{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2d6558",
   "metadata": {},
   "source": [
    "# REPRODUCIBILITY PACKAGE FOR:\n",
    "## Development acupuncture: The network structure of multidimensional poverty and its implications\n",
    "\n",
    "This notebook can be used to reproduce the results of the working paper \"Development acupuncture: The network structure of multidimensional poverty and its implications\" by Viktor Stojkoski , Luis F. Lopez-Calva,  Kimberly Bolch , and Almudena Fernandez.\n",
    "\n",
    "All calculations were done in Python 3.9.13 and require the following packages:\n",
    "\n",
    "* numpy: 1.26.4\n",
    "\n",
    "* pandas: 2.2.0\n",
    "\n",
    "* scipy: 1.12.0\n",
    "\n",
    "* statsmodels: 0.14.1\n",
    "\n",
    "* matplotlib: 3.8.3\n",
    "\n",
    "* plotly: 5.9.0\n",
    "\n",
    "* networkx: 2.8.4\n",
    "\n",
    "* sklearn: 1.0.2\n",
    "\n",
    "* linearmodels: 5.4\n",
    "\n",
    "* stargazer==0.0.6\n",
    "\n",
    "* policy_priority_inference.py (imported from the .py file in the same directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2ae9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "from stargazer.stargazer import Stargazer\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import networkx as nx\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import policy_priority_inference as ppi\n",
    "\n",
    "\n",
    "\n",
    "def remove_row_col(matrix, vec, var_names, variable):\n",
    "    index = var_names.index(variable)\n",
    "    new_matrix = np.delete(matrix, index, axis=0)  # Remove row\n",
    "    new_matrix = np.delete(new_matrix, index, axis=1)  # Remove column\n",
    "    new_vec = np.delete(vec, index)\n",
    "    return new_matrix, vec\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00c546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>MPI</th>\n",
       "      <th>var_sample</th>\n",
       "      <th>A_int</th>\n",
       "      <th>A_avg</th>\n",
       "      <th>A_avg_raw</th>\n",
       "      <th>A_int_rank</th>\n",
       "      <th>centrality_w_norm</th>\n",
       "      <th>centrality_w_norm_rank</th>\n",
       "      <th>full_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nam</td>\n",
       "      <td>7</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_cm_01</td>\n",
       "      <td>0.005434</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.376074</td>\n",
       "      <td>10</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>10</td>\n",
       "      <td>Child mortality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nam</td>\n",
       "      <td>7</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_nutr_01</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>0.142377</td>\n",
       "      <td>0.347675</td>\n",
       "      <td>5</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>6</td>\n",
       "      <td>Nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nam</td>\n",
       "      <td>7</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_satt_01</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>0.064793</td>\n",
       "      <td>0.368351</td>\n",
       "      <td>8</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>9</td>\n",
       "      <td>School attendance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nam</td>\n",
       "      <td>7</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_educ_01</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>0.369037</td>\n",
       "      <td>9</td>\n",
       "      <td>0.039208</td>\n",
       "      <td>8</td>\n",
       "      <td>Schooling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nam</td>\n",
       "      <td>7</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_elct_01</td>\n",
       "      <td>0.062034</td>\n",
       "      <td>0.273219</td>\n",
       "      <td>0.319473</td>\n",
       "      <td>3</td>\n",
       "      <td>0.160981</td>\n",
       "      <td>3</td>\n",
       "      <td>Electricity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_wtr_01</td>\n",
       "      <td>0.062204</td>\n",
       "      <td>0.347358</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>5</td>\n",
       "      <td>0.114861</td>\n",
       "      <td>5</td>\n",
       "      <td>Drinking water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_sani_01</td>\n",
       "      <td>0.093881</td>\n",
       "      <td>0.450380</td>\n",
       "      <td>0.462862</td>\n",
       "      <td>2</td>\n",
       "      <td>0.151528</td>\n",
       "      <td>2</td>\n",
       "      <td>Sanitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_hsg_01</td>\n",
       "      <td>0.074373</td>\n",
       "      <td>0.401509</td>\n",
       "      <td>0.482370</td>\n",
       "      <td>3</td>\n",
       "      <td>0.133203</td>\n",
       "      <td>3</td>\n",
       "      <td>Housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_ckfl_01</td>\n",
       "      <td>0.099045</td>\n",
       "      <td>0.456024</td>\n",
       "      <td>0.457698</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154431</td>\n",
       "      <td>1</td>\n",
       "      <td>Cooking fuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_asst_01</td>\n",
       "      <td>0.054067</td>\n",
       "      <td>0.320666</td>\n",
       "      <td>0.502676</td>\n",
       "      <td>6</td>\n",
       "      <td>0.105662</td>\n",
       "      <td>6</td>\n",
       "      <td>Assets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1472 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country  year       MPI var_sample     A_int     A_avg  A_avg_raw  \\\n",
       "0        nam     7  0.381508    d_cm_01  0.005434  0.024324   0.376074   \n",
       "1        nam     7  0.381508  d_nutr_01  0.033832  0.142377   0.347675   \n",
       "2        nam     7  0.381508  d_satt_01  0.013156  0.064793   0.368351   \n",
       "3        nam     7  0.381508  d_educ_01  0.012470  0.066746   0.369037   \n",
       "4        nam     7  0.381508  d_elct_01  0.062034  0.273219   0.319473   \n",
       "...      ...   ...       ...        ...       ...       ...        ...   \n",
       "1467     mdg    18  0.556743   d_wtr_01  0.062204  0.347358   0.494539   \n",
       "1468     mdg    18  0.556743  d_sani_01  0.093881  0.450380   0.462862   \n",
       "1469     mdg    18  0.556743   d_hsg_01  0.074373  0.401509   0.482370   \n",
       "1470     mdg    18  0.556743  d_ckfl_01  0.099045  0.456024   0.457698   \n",
       "1471     mdg    18  0.556743  d_asst_01  0.054067  0.320666   0.502676   \n",
       "\n",
       "      A_int_rank  centrality_w_norm  centrality_w_norm_rank         full_names  \n",
       "0             10           0.014261                      10    Child mortality  \n",
       "1              5           0.084066                       6          Nutrition  \n",
       "2              8           0.038085                       9  School attendance  \n",
       "3              9           0.039208                       8          Schooling  \n",
       "4              3           0.160981                       3        Electricity  \n",
       "...          ...                ...                     ...                ...  \n",
       "1467           5           0.114861                       5     Drinking water  \n",
       "1468           2           0.151528                       2         Sanitation  \n",
       "1469           3           0.133203                       3            Housing  \n",
       "1470           1           0.154431                       1       Cooking fuel  \n",
       "1471           6           0.105662                       6             Assets  \n",
       "\n",
       "[1472 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT_FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6e2f9",
   "metadata": {},
   "source": [
    "## CLEAR DATA: ESTIMATE THE POVERTY SPACE & POVERTY CENTRALITY\n",
    "\n",
    "The code below clears the raw data and estimates the Poverty Space and Poverty Centrality for all countries in our data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c8e960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "## CLEAR DATA: ESTIMATE THE POVERTY SPACE & POVERTY CENTRALITY\n",
    "\n",
    "\n",
    "var_weights = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "variables = ['d_cm_01', 'd_nutr_01', 'd_satt_01', 'd_educ_01', 'd_elct_01',\n",
    "             'd_wtr_01', 'd_sani_01', 'd_hsg_01', 'd_ckfl_01', 'd_asst_01']\n",
    "dimensions = [1, 1, 2, 2, 3, 3, 3, 3, 3, 3]\n",
    "full_variable_names = ['Child mortality', 'Nutrition', 'School attendance', 'Schooling',\n",
    "                       'Electricity', 'Drinking water', 'Sanitation', 'Housing',\n",
    "                       'Cooking fuel', 'Assets']\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "# In Python, using glob, you can search for all CSV files in the current working directory\n",
    "\n",
    "search_pattern = os.path.join(os.getcwd(), 'data', '*.csv')\n",
    "csv_files = glob.glob(search_pattern)\n",
    "\n",
    "PHI_NORMALIZED = {}\n",
    "list_names = {}\n",
    "list_short_names = {}\n",
    "\n",
    "Correlation_matrix = np.nan * np.zeros((len(csv_files), len(variables)))\n",
    "\n",
    "                                       \n",
    "columns = ['country', 'year', 'MPI', 'var_sample', 'A_int', 'A_avg', 'A_avg_raw', 'A_int_rank', 'centrality_w_norm', 'centrality_w_norm_rank', 'full_names']\n",
    "TT_FINAL = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in range(len(csv_files)):\n",
    "\n",
    "    # Get the file name from the path\n",
    "    filename = os.path.basename(csv_files[i])\n",
    "\n",
    "    # Extract country code and year from the filename\n",
    "    ctry = filename[:3]\n",
    "    yyr = filename[-6:-4]  # Adjust according to your file naming convention if needed\n",
    "\n",
    "    # Read the data\n",
    "    TT = pd.read_csv(csv_files[i])\n",
    "\n",
    "    # Create the deprivation matrix\n",
    "    XX = pd.DataFrame()\n",
    "    jj = []\n",
    "\n",
    "    for j, var in enumerate(variables):\n",
    "        if var in TT.columns:\n",
    "            X = TT[var]\n",
    "            if X.isna().mean() < 1:  # Check if the column is not entirely NaN\n",
    "                XX = pd.concat([XX, X], axis=1)\n",
    "            else:\n",
    "                jj.append(j)  # Append j+1 to maintain MATLAB's 1-based indexing\n",
    "\n",
    "    # Assuming previous steps have been correctly executed\n",
    "\n",
    "    # Convert XX back to DataFrame if not already, to simplify handling NaN values\n",
    "    # This step is precautionary; XX should already be a DataFrame after previous operations\n",
    "    XX = pd.DataFrame(XX)\n",
    "\n",
    "    # Filter out rows with any NaN values across all columns in XX\n",
    "    # Also filter corresponding rows in the weights array\n",
    "    valid_rows = ~XX.isnull().any(axis=1)\n",
    "    XX_filtered = XX[valid_rows]\n",
    "\n",
    "    weights = TT['weight'].values  # Extracting weight values from the DataFrame\n",
    "    weights_sample = np.array(var_weights)  # Ensure var_weights is a numpy array for easy manipulation\n",
    "\n",
    "    # Adjust weights_sample based on jj, ensuring jj is zero-based\n",
    "    weights_sample = np.delete(weights_sample, jj)\n",
    "\n",
    "    weights_filtered = weights[valid_rows]\n",
    "\n",
    "    var_sample = [var for i, var in enumerate(variables) if i not in jj]\n",
    "    full_names = [name for i, name in enumerate(full_variable_names) if i not in jj]\n",
    "\n",
    "\n",
    "    \n",
    "    # Normalize the sample weights\n",
    "    weights_sample_normalized = weights_sample / sum(weights_sample)\n",
    "\n",
    "    # Calculate the weighted deprivation matrix G\n",
    "    # Note: Element-wise multiplication of weights_sample_normalized with each column in XX_filtered\n",
    "    G = XX_filtered.multiply(weights_sample_normalized, axis='columns')\n",
    "\n",
    "    # Calculate Z based on the threshold condition\n",
    "    Z = np.where(G.sum(axis=1) > threshold, 1, 0) if threshold == 0 else np.where(G.sum(axis=1) >= threshold, 1, 0)\n",
    "\n",
    "    # Calculate the final index H\n",
    "    H = np.sum(weights_filtered * Z) / np.sum(weights_filtered)    \n",
    "\n",
    "\n",
    "    # Ensure G is a numpy array for these operations, if not already\n",
    "    G = G.to_numpy() if hasattr(G, 'to_numpy') else G\n",
    "\n",
    "    # Recreate G_k from G\n",
    "    G_k = np.copy(G)\n",
    "\n",
    "    # Initialize xx with zeros with the correct dimensions\n",
    "    # Note: Adjusting approach to ensure dimensions match expectations for numpy operations\n",
    "    xx = np.zeros((np.sum(1 - Z), G_k.shape[1]))\n",
    "\n",
    "    # Update G_k directly with numpy where applicable\n",
    "    # We need to ensure we're updating rows where Z is 0 with xx, but this approach requires boolean indexing or equivalent in numpy\n",
    "    not_Z_rows = np.where(Z == 0)[0]  # Get indices of rows where Z is 0\n",
    "\n",
    "    # Since direct assignment as attempted can mismatch dimensions, we ensure alignment before attempting to replace\n",
    "    # However, given xx aims to replace all not_Z_rows with zeros, we can simplify by directly setting zeros without using xx\n",
    "    G_k[not_Z_rows, :] = 0  # Directly set rows where Z is 0 to zeros, aligning with intention\n",
    "\n",
    "    # Proceed with adjusted calculations\n",
    "    # Note: The calculations for W, A, A_est, and MPI should then ensure they are using numpy operations as well\n",
    "\n",
    "    W_filtered = weights_filtered.reshape(-1, 1)  # Ensure weights_filtered is column vector for matrix operations\n",
    "    A = (W_filtered * G_k) / np.sum(weights_filtered[Z == 1])\n",
    "\n",
    "    A_est = np.sum(A, axis=0)\n",
    "\n",
    "    MPI = np.sum(A_est * H)\n",
    "\n",
    "    A_est *= H  # Update A_est by multiplying it with H\n",
    "\n",
    "    # Create weighted networks\n",
    "    Z_indices = np.where(Z)[0]  # Find indices where Z is True\n",
    "    G_z = XX_filtered.iloc[Z_indices, :].to_numpy()  # Filter G_z based on Z\n",
    "\n",
    "    # Ensure 'weights' and 'Z' are numpy arrays for the operations\n",
    "    weights_final = np.array(weights_filtered)\n",
    "    Z = np.array(Z, dtype=bool)\n",
    "    A = np.array(A)  # Assuming 'A' is already correctly shaped and matches 'Z'\n",
    "\n",
    "    # Select rows from 'A' where 'Z' is True\n",
    "    A_filtered = A[Z, :]\n",
    "\n",
    "    # Calculate the dot product of 'weights' transposed and 'Z' (which should be a scalar if both are vectors)\n",
    "    weights_Z_product = np.dot(weights_final.T, Z)\n",
    "\n",
    "    # Scale 'A_filtered' by 'weights_Z_product'\n",
    "    M_w = weights_Z_product * A_filtered ## NOT CALCULATED CORRECTLY!!!!\n",
    "\n",
    "    # Remove columns where the sum across rows in G_z is 0\n",
    "    xx = np.sum(G_z > 0, axis=0)\n",
    "    jj = np.where(xx == 0)[0]\n",
    "\n",
    "    # Update var_sample and full_names by removing indices jj\n",
    "    var_sample = np.delete(np.array(var_sample), jj).tolist()\n",
    "    full_names = np.delete(np.array(full_names), jj).tolist()\n",
    "\n",
    "    # Update G_z, M_w, and M_u by removing columns jj\n",
    "    G_z = np.delete(G_z, jj, axis=1)\n",
    "    M_w = np.delete(M_w, jj, axis=1)\n",
    "\n",
    "    # Create similarity networks\n",
    "    PHI_w_int = np.dot(M_w.T, G_z)  # Calculate initial similarity matrix\n",
    "    PHI_w_norm = np.full(PHI_w_int.shape, np.nan)  # Initialize PHI_w_norm with NaN\n",
    "\n",
    "    # Normalize PHI_w_int to create PHI_w_norm\n",
    "    for k in range(PHI_w_int.shape[0]):\n",
    "        for j in range(PHI_w_int.shape[1]):\n",
    "            PHI_w_norm[k, j] = PHI_w_int[k, j] / PHI_w_int[j, j]\n",
    "\n",
    "    # Adjust PHI_w_norm by subtracting the identity matrix scaled by PHI_w_norm\n",
    "    PHI_w_norm = PHI_w_norm - np.eye(PHI_w_norm.shape[0]) * PHI_w_norm\n",
    "\n",
    "    D, V = np.linalg.eig(PHI_w_norm)\n",
    "\n",
    "    # Find the index of the eigenvalue with the maximum magnitude\n",
    "    z = np.argmax(D)\n",
    "\n",
    "    # Extract the corresponding eigenvector and compute the centrality measure\n",
    "    centrality_w_norm = V[:, z]\n",
    "\n",
    "    # Normalize the centrality measure so that its sum equals 1\n",
    "    centrality_w_norm /= np.sum(centrality_w_norm)\n",
    "    \n",
    "    for j in range(len(variables)):\n",
    "        \n",
    "        try:\n",
    "            index = var_sample.index(variables[j])\n",
    "        except ValueError:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        Phi_reduced = np.delete(PHI_w_norm, index, axis=0).copy()  # Remove row\n",
    "        Phi_reduced = np.delete(Phi_reduced, index, axis=1)  # Remove column\n",
    "        centrality_reduced = np.delete(centrality_w_norm, index).copy()\n",
    "        \n",
    "        D, V = np.linalg.eig(Phi_reduced)\n",
    "\n",
    "        # Find the index of the eigenvalue with the maximum magnitude\n",
    "        z = np.argmax(D)\n",
    "\n",
    "        # Extract the corresponding eigenvector and compute the centrality measure\n",
    "        centrality_w_norm_reduced = V[:, z]\n",
    "\n",
    "        # Normalize the centrality measure so that its sum equals 1\n",
    "        centrality_w_norm_reduced /= np.sum(centrality_w_norm_reduced)\n",
    "        centrality_reduced /= np.sum(centrality_reduced)\n",
    "        \n",
    "        \n",
    "        # Calculate the correlation and assign it to the Correlation_matrix\n",
    "        corr_matrix = np.corrcoef(centrality_w_norm_reduced, centrality_reduced)\n",
    "        # corr_matrix[0, 1] or corr_matrix[1, 0] contains the correlation coefficient\n",
    "        Correlation_matrix[i, j] = corr_matrix[0, 1]\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    # Assuming ctry, yyr, MPI, var_sample, centrality_w_norm, A_est, PHI_w_norm, and full_names are defined\n",
    "\n",
    "    num_vars = len(var_sample)\n",
    "\n",
    "    # 1. Replicating values\n",
    "    country = np.repeat(ctry, num_vars)\n",
    "    year = np.repeat(yyr, num_vars)\n",
    "    MPI_repeated = np.repeat(MPI, num_vars)\n",
    "\n",
    "    # 2. Sorting and ranking centrality_w_norm\n",
    "    centrality_w_norm_rank = rankdata(-centrality_w_norm, method='ordinal')\n",
    "\n",
    "\n",
    "    # Sorting A_int and ranking\n",
    "    A_int = A_est.T\n",
    "    A_int = np.delete(A_int, jj)  # Remove indices as specified by jj\n",
    "    A_int_rank = rankdata(-A_int, method='ordinal')\n",
    "\n",
    "\n",
    "    # 3. Calculating Averages\n",
    "    A_avg = PHI_w_norm @ A_int\n",
    "    PHI_ones = np.ones_like(PHI_w_norm)\n",
    "    PHI_ones -= np.eye(len(A_avg))\n",
    "    A_avg_raw = PHI_ones @ A_int\n",
    "\n",
    "    # 4. Creating the results table\n",
    "    # Note: var_sample, centrality_w_norm, and full_names should be lists or arrays of matching length\n",
    "    TT_results = pd.DataFrame({\n",
    "        'country': country,\n",
    "        'year': year,\n",
    "        'MPI': MPI_repeated,\n",
    "        'var_sample': var_sample,\n",
    "        'A_int': A_int,\n",
    "        'A_avg': A_avg,\n",
    "        'A_avg_raw': A_avg_raw,\n",
    "        'A_int_rank': A_int_rank,  # Adjust for zero-based indexing in Python\n",
    "        'centrality_w_norm': centrality_w_norm,\n",
    "        'centrality_w_norm_rank': centrality_w_norm_rank,\n",
    "        'full_names': full_names\n",
    "    })\n",
    "\n",
    "    # Dynamically setting values in dictionaries using the current value of i\n",
    "    PHI_NORMALIZED[f'phi_{ctry}_{yyr}'] = PHI_w_norm\n",
    "    list_names[f'variable_names_{ctry}_{yyr}'] = full_names\n",
    "    list_short_names[f'variable_names_{ctry}_{yyr}'] = var_sample\n",
    "\n",
    "\n",
    "    TT_FINAL = pd.concat([TT_FINAL, TT_results], ignore_index=True)\n",
    "    \n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75dfce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_country_year</th>\n",
       "      <th>country_full</th>\n",
       "      <th>year_full</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>MPI</th>\n",
       "      <th>var_sample</th>\n",
       "      <th>A_int</th>\n",
       "      <th>A_avg</th>\n",
       "      <th>A_avg_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>centrality_w_norm</th>\n",
       "      <th>centrality_w_norm_rank</th>\n",
       "      <th>full_names</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Namibia 2007</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>2007</td>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_cm_01</td>\n",
       "      <td>0.005434</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.376074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>10</td>\n",
       "      <td>Child mortality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Namibia 2007</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>2007</td>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_nutr_01</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>0.142377</td>\n",
       "      <td>0.347675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>6</td>\n",
       "      <td>Nutrition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Namibia 2007</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>2007</td>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_satt_01</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>0.064793</td>\n",
       "      <td>0.368351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>9</td>\n",
       "      <td>School attendance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Namibia 2007</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>2007</td>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_educ_01</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>0.369037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039208</td>\n",
       "      <td>8</td>\n",
       "      <td>Schooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASM</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Namibia 2007</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>2007</td>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_elct_01</td>\n",
       "      <td>0.062034</td>\n",
       "      <td>0.273219</td>\n",
       "      <td>0.319473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160981</td>\n",
       "      <td>3</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AND</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Madagascar 2018</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2018</td>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_wtr_01</td>\n",
       "      <td>0.062204</td>\n",
       "      <td>0.347358</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114861</td>\n",
       "      <td>5</td>\n",
       "      <td>Drinking water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>Madagascar 2018</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2018</td>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_sani_01</td>\n",
       "      <td>0.093881</td>\n",
       "      <td>0.450380</td>\n",
       "      <td>0.462862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151528</td>\n",
       "      <td>2</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>Madagascar 2018</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2018</td>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_hsg_01</td>\n",
       "      <td>0.074373</td>\n",
       "      <td>0.401509</td>\n",
       "      <td>0.482370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133203</td>\n",
       "      <td>3</td>\n",
       "      <td>Housing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>Madagascar 2018</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2018</td>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_ckfl_01</td>\n",
       "      <td>0.099045</td>\n",
       "      <td>0.456024</td>\n",
       "      <td>0.457698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154431</td>\n",
       "      <td>1</td>\n",
       "      <td>Cooking fuel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>Madagascar 2018</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2018</td>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_asst_01</td>\n",
       "      <td>0.054067</td>\n",
       "      <td>0.320666</td>\n",
       "      <td>0.502676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105662</td>\n",
       "      <td>6</td>\n",
       "      <td>Assets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1472 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     full_country_year country_full  year_full country year       MPI  \\\n",
       "0         Namibia 2007      Namibia       2007     nam   07  0.381508   \n",
       "1         Namibia 2007      Namibia       2007     nam   07  0.381508   \n",
       "2         Namibia 2007      Namibia       2007     nam   07  0.381508   \n",
       "3         Namibia 2007      Namibia       2007     nam   07  0.381508   \n",
       "4         Namibia 2007      Namibia       2007     nam   07  0.381508   \n",
       "...                ...          ...        ...     ...  ...       ...   \n",
       "1467   Madagascar 2018   Madagascar       2018     mdg   18  0.556743   \n",
       "1468   Madagascar 2018   Madagascar       2018     mdg   18  0.556743   \n",
       "1469   Madagascar 2018   Madagascar       2018     mdg   18  0.556743   \n",
       "1470   Madagascar 2018   Madagascar       2018     mdg   18  0.556743   \n",
       "1471   Madagascar 2018   Madagascar       2018     mdg   18  0.556743   \n",
       "\n",
       "     var_sample     A_int     A_avg  A_avg_raw  ...  centrality_w_norm  \\\n",
       "0       d_cm_01  0.005434  0.024324   0.376074  ...           0.014261   \n",
       "1     d_nutr_01  0.033832  0.142377   0.347675  ...           0.084066   \n",
       "2     d_satt_01  0.013156  0.064793   0.368351  ...           0.038085   \n",
       "3     d_educ_01  0.012470  0.066746   0.369037  ...           0.039208   \n",
       "4     d_elct_01  0.062034  0.273219   0.319473  ...           0.160981   \n",
       "...         ...       ...       ...        ...  ...                ...   \n",
       "1467   d_wtr_01  0.062204  0.347358   0.494539  ...           0.114861   \n",
       "1468  d_sani_01  0.093881  0.450380   0.462862  ...           0.151528   \n",
       "1469   d_hsg_01  0.074373  0.401509   0.482370  ...           0.133203   \n",
       "1470  d_ckfl_01  0.099045  0.456024   0.457698  ...           0.154431   \n",
       "1471  d_asst_01  0.054067  0.320666   0.502676  ...           0.105662   \n",
       "\n",
       "      centrality_w_norm_rank         full_names Unnamed: 14  Unnamed: 15  \\\n",
       "0                         10    Child mortality         NaN          NaN   \n",
       "1                          6          Nutrition         NaN          NaN   \n",
       "2                          9  School attendance         NaN          NaN   \n",
       "3                          8          Schooling         NaN          NaN   \n",
       "4                          3        Electricity         NaN          NaN   \n",
       "...                      ...                ...         ...          ...   \n",
       "1467                       5     Drinking water         NaN          NaN   \n",
       "1468                       2         Sanitation         NaN          NaN   \n",
       "1469                       3            Housing         NaN          NaN   \n",
       "1470                       1       Cooking fuel         NaN          NaN   \n",
       "1471                       6             Assets         NaN          NaN   \n",
       "\n",
       "      Unnamed: 16     Unnamed: 17 Unnamed: 18  Unnamed: 19  Unnamed: 20  \n",
       "0             AFG     Afghanistan         NaN          NaN          NaN  \n",
       "1             ALB         Albania         NaN          NaN          NaN  \n",
       "2             DZA         Algeria         NaN          NaN          NaN  \n",
       "3             ASM  American Samoa         NaN          NaN          NaN  \n",
       "4             AND         Andorra         NaN          NaN          NaN  \n",
       "...           ...             ...         ...          ...          ...  \n",
       "1467          NaN             NaN         NaN          NaN          NaN  \n",
       "1468          NaN             NaN         NaN          NaN          NaN  \n",
       "1469          NaN             NaN         NaN          NaN          NaN  \n",
       "1470          NaN             NaN         NaN          NaN          NaN  \n",
       "1471          NaN             NaN         NaN          NaN          NaN  \n",
       "\n",
       "[1472 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT_FINAL = pd.read_csv('TT_FINAL.csv', encoding='latin1')\n",
    "TT_FINAL['year'] = TT_FINAL['year'].astype(str).str.zfill(2)\n",
    "ctry_yr = 'Kyrgyz Republic 2006'\n",
    "rows = TT_FINAL[TT_FINAL['full_country_year'] == ctry_yr].copy()\n",
    "ctry_init = rows['country'].unique()\n",
    "yyr_init = rows['year'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f55c9",
   "metadata": {},
   "source": [
    "## GENERATE DATA FOR REGRESSION ANALYSIS\n",
    "\n",
    "The code below generates the data used in the regression analysis (Tables 1-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c918795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "## GENERATE DATA FOR REGRESSION ANALYSIS - TABLE 1\n",
    "\n",
    "unique_pairs = TT_FINAL[['country', 'year']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Count occurrences of each unique entry in 'country'\n",
    "count_occurrences = TT_FINAL['country'].value_counts()\n",
    "\n",
    "# Initialize an empty DataFrame for regression data\n",
    "TT_regression = pd.DataFrame()\n",
    "\n",
    "# Iterate over each unique country with more than 10 occurrences\n",
    "for country in count_occurrences[count_occurrences > 10].index:\n",
    "    \n",
    "    # Filter TT_FINAL for the current country\n",
    "    TT_int = TT_FINAL[TT_FINAL['country'] == country].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Convert 'year' to numeric and get unique years\n",
    "    years = pd.to_numeric(TT_int['year'], errors='coerce')\n",
    "    unique_years = years.unique()\n",
    "    \n",
    "    # Find the minimum and maximum years\n",
    "    min_year = TT_int['year'].min()\n",
    "    max_year = TT_int['year'].max()\n",
    "    \n",
    "    # Find all indices where 'year' is equal to the min_year and max_year\n",
    "    z_initial = TT_int[TT_int['year'] == min_year].index\n",
    "    z_final = TT_int[TT_int['year'] == max_year].index\n",
    "    \n",
    "    \n",
    "    names_initial = TT_int.loc[z_initial, 'var_sample']\n",
    "    names_final = TT_int.loc[z_final, 'var_sample']\n",
    "\n",
    "    # Finding the intersection of 'var_sample' names between initial and final\n",
    "    vars = set(names_initial) & set(names_final)\n",
    "\n",
    "    # Define empty lists to store the indices\n",
    "    zz_initial = []\n",
    "    zz_final = []\n",
    "\n",
    "    # Iterate over each variable name in 'vars' to find its occurrences within the initial and final years\n",
    "    for var in vars:\n",
    "        # Filter TT_int for rows where 'var_sample' matches the current variable name\n",
    "        matching_rows = TT_int[TT_int['var_sample'] == var].index\n",
    "\n",
    "        # Intersect matching_rows with z_initial and z_final using pandas Index intersection\n",
    "        intersection_initial = matching_rows.intersection(z_initial)\n",
    "        intersection_final = matching_rows.intersection(z_final)\n",
    "\n",
    "        # Store the first index of the intersection if it exists; otherwise, store NaN\n",
    "        zz_initial.append(intersection_initial[0] if not intersection_initial.empty else np.nan)\n",
    "        zz_final.append(intersection_final[0] if not intersection_final.empty else np.nan)\n",
    "\n",
    "    # Convert lists to pandas Index for further consistency in indexing if needed\n",
    "    zz_initial = pd.Index(zz_initial).dropna().astype(int)\n",
    "    zz_final = pd.Index(zz_final).dropna().astype(int)\n",
    "            \n",
    "    # Extract the initial country-year pair\n",
    "    initial_pair = TT_int.iloc[z_initial][['country', 'year']]\n",
    "\n",
    "    # Assuming 'unique_pairs' is a DataFrame with a reset index\n",
    "    # Find the row in 'unique_pairs' that matches 'initial_pair'\n",
    "    phi_loc = unique_pairs.reset_index().merge(initial_pair, on=['country', 'year'], how='inner')['index'].values[0]\n",
    "\n",
    "    # Using the found index (phi_loc) to access the relevant data\n",
    "    PHI_initial_key = f'phi_{country}_{min_year}'\n",
    "    short_var_name_key = f'variable_names_{country}_{min_year}'\n",
    "\n",
    "    PHI_initial = PHI_NORMALIZED.get(PHI_initial_key, None)\n",
    "    short_var_names = list_short_names.get(short_var_name_key, [])\n",
    "\n",
    "    # Filter PHI_initial based on the intersection of vars and short_var_names\n",
    "    if PHI_initial is not None:\n",
    "        # Find indices of vars in short_var_names\n",
    "        indices = [short_var_names.index(var) for var in vars if var in short_var_names]\n",
    "\n",
    "        # Filter PHI_initial using numpy's advanced indexing\n",
    "        PHI_initial_filtered = PHI_initial[np.ix_(indices, indices)]\n",
    "        \n",
    "        \n",
    "    # Calculate changes for each metric between initial and final observations\n",
    "    change_MPI = TT_int.loc[zz_final, 'MPI'].values - TT_int.loc[zz_initial, 'MPI'].values\n",
    "    change_A = TT_int.loc[zz_final, 'A_int'].values - TT_int.loc[zz_initial, 'A_int'].values\n",
    "    centrality_w_norm_final = TT_int.loc[zz_final, 'centrality_w_norm'].values\n",
    "    # Assuming change_A is a single value or a numpy array that matches the dimensions for multiplication with PHI_initial\n",
    "    # If PHI_initial is a DataFrame, ensure it's converted to a numpy array for this operation, if not already compatible\n",
    "    \n",
    "    # Calculate the number of years between the initial and final observations\n",
    "    # Assuming unique_yyrs is a numpy array or a pandas Series of unique years present in TT_int\n",
    "    change_years = max(unique_years) - min(unique_years)\n",
    "\n",
    "    # Normalize the changes by the number of years\n",
    "    # These operations assume change_years is a scalar. If change_years is an array, adjustments might be needed\n",
    "    change_A /= change_years\n",
    "    \n",
    "    \n",
    "    # Prepare the DataFrame for regression analysis by selecting the initial observation\n",
    "    TT_int_for_regression = TT_int.loc[zz_initial, :].copy()\n",
    "\n",
    "    # Add calculated changes to the DataFrame\n",
    "    TT_int_for_regression['change_MPI'] = change_MPI\n",
    "    TT_int_for_regression['change_A'] = change_A\n",
    "    TT_int_for_regression['change_years'] = change_years\n",
    "    TT_int_for_regression['centrality_w_norm_final'] = centrality_w_norm_final\n",
    "\n",
    "    # Append the prepared DataFrame to the TT_regression DataFrame\n",
    "    TT_regression = pd.concat([TT_regression, TT_int_for_regression], ignore_index=True)\n",
    "    \n",
    "\n",
    "## ADD INSTRUMENTAL\n",
    "\n",
    "\n",
    "unique_regression_pairs = TT_regression[['country', 'year']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Initialize a matrix to hold the instrumental correlations\n",
    "CORRELATION_INSTRUMENTAL = np.nan * np.zeros((len(unique_regression_pairs), len(unique_regression_pairs)))\n",
    "\n",
    "for i in range(len(unique_regression_pairs)):\n",
    "    for j in range(len(unique_regression_pairs)):\n",
    "        # Dynamically construct keys based on country and year\n",
    "        key_i = f\"phi_{unique_regression_pairs.iloc[i]['country']}_{unique_regression_pairs.iloc[i]['year']}\"\n",
    "        key_j = f\"phi_{unique_regression_pairs.iloc[j]['country']}_{unique_regression_pairs.iloc[j]['year']}\"\n",
    "        names_key_i = f\"variable_names_{unique_regression_pairs.iloc[i]['country']}_{unique_regression_pairs.iloc[i]['year']}\"\n",
    "        names_key_j = f\"variable_names_{unique_regression_pairs.iloc[j]['country']}_{unique_regression_pairs.iloc[j]['year']}\"\n",
    "\n",
    "        \n",
    "        PHI_i = PHI_NORMALIZED[key_i]\n",
    "        names_i = list_short_names[names_key_i]\n",
    "\n",
    "        PHI_j = PHI_NORMALIZED[key_j]\n",
    "        names_j = list_short_names[names_key_j]\n",
    "\n",
    "        # Find common variables\n",
    "        vars = list(set(names_i) & set(names_j))\n",
    "        \n",
    "        # Indices of common variables in PHI matrices\n",
    "        zz_1 = [names_i.index(var) for var in vars]\n",
    "        zz_2 = [names_j.index(var) for var in vars]\n",
    "        \n",
    "        # Selecting the relevant rows and columns from PHI matrices based on common variables\n",
    "        PHI_i_reduced = PHI_i[np.ix_(zz_1, zz_1)]\n",
    "        PHI_j_reduced = PHI_j[np.ix_(zz_2, zz_2)]\n",
    "        \n",
    "        # Calculate correlation between flattened matrices\n",
    "        corr_value = np.corrcoef(PHI_i_reduced.flatten(), PHI_j_reduced.flatten())[0, 1]\n",
    "        CORRELATION_INSTRUMENTAL[i, j] = corr_value\n",
    "\n",
    "  #  print(f\"Processed pair {i+1} of {len(unique_regression_pairs)}\")\n",
    "        \n",
    "# Adjust the correlation matrix\n",
    "CORRELATION_INSTRUMENTAL -= np.eye(len(unique_regression_pairs))\n",
    "\n",
    "# Find max correlation indices\n",
    "max_indices = np.argmax(CORRELATION_INSTRUMENTAL, axis=1)\n",
    "\n",
    "# Initialize centrality_w_norm_inst as NaNs\n",
    "centrality_w_norm_inst = np.nan * np.ones(len(TT_regression['centrality_w_norm']))\n",
    "\n",
    "for i in range(len(unique_regression_pairs)):\n",
    "    country_i, year_i = unique_regression_pairs.iloc[i]['country'], unique_regression_pairs.iloc[i]['year']\n",
    "    country_j, year_j = unique_regression_pairs.iloc[max_indices[i]]['country'], unique_regression_pairs.iloc[max_indices[i]]['year']\n",
    "\n",
    "    # Filter TT_regression_2 for the current and max correlated pairs\n",
    "    pair_i = TT_regression[(TT_regression['country'] == country_i) & (TT_regression['year'] == year_i)]\n",
    "    pair_j = TT_regression[(TT_regression['country'] == country_j) & (TT_regression['year'] == year_j)]\n",
    "\n",
    "    # Common variables\n",
    "    vars = list(set(pair_i['var_sample']) & set(pair_j['var_sample']))\n",
    "\n",
    "    # Find indices in TT_regression_2 for vars in both pairs\n",
    "    for var in vars:\n",
    "        index_i = pair_i[pair_i['var_sample'] == var].index\n",
    "        index_j = pair_j[pair_j['var_sample'] == var].index\n",
    "\n",
    "        # Update centrality_w_norm_inst based on common variables\n",
    "        centrality_w_norm_inst[index_i] = pair_j.loc[index_j, 'centrality_w_norm'].values\n",
    "    \n",
    "TT_regression['centrality_w_norm_inst'] = centrality_w_norm_inst\n",
    "\n",
    "## GENERATE DATA FOR 3 PERIOD REGRESSION\n",
    "\n",
    "# Get unique pairs from the first two columns\n",
    "unique_pairs = TT_FINAL.iloc[:, 0:2].drop_duplicates()\n",
    "\n",
    "# Convert country column to string type, find unique entries and their counts\n",
    "unique_entries, entry_counts = np.unique(TT_FINAL['country'].astype(str), return_counts=True)\n",
    "count_occurrences = np.bincount(entry_counts)\n",
    "\n",
    "TT_regression_3_period = pd.DataFrame()\n",
    "for i, unique_entry in enumerate(unique_entries):\n",
    "    if (unique_pairs['country'] == unique_entry).sum() == 3:\n",
    "        indices = TT_FINAL[TT_FINAL['country'] == unique_entry].index\n",
    "        TT_int = TT_FINAL.loc[indices]\n",
    "\n",
    "        # Convert 'year' to numeric and get unique years\n",
    "        years = pd.to_numeric(TT_int['year'], errors='coerce')\n",
    "        unique_years = np.sort(years.unique())\n",
    "    \n",
    "\n",
    "        for KK in range(len(unique_years) - 1):\n",
    "            indices_initial = TT_int[years == unique_years[KK]].index\n",
    "            indices_final = TT_int[years == unique_years[KK + 1]].index\n",
    "\n",
    "            names_initial = TT_int.loc[indices_initial, 'var_sample']\n",
    "            names_final = TT_int.loc[indices_final, 'var_sample']\n",
    "\n",
    "            vars = np.intersect1d(names_initial, names_final)\n",
    "            zz_initial = np.full(vars.shape, np.nan)\n",
    "            zz_final = np.full(vars.shape, np.nan)\n",
    "\n",
    "            for j, var in enumerate(vars):\n",
    "                z = TT_int[TT_int['var_sample'] == var].index\n",
    "                zz_initial[j] = np.intersect1d(indices_initial, z)\n",
    "                zz_final[j] = np.intersect1d(indices_final, z)\n",
    "\n",
    "            initial_pair = TT_int.loc[indices_initial, :].iloc[:, 0:2].drop_duplicates()\n",
    "            \n",
    "             # Using the found index (phi_loc) to access the relevant data\n",
    "            PHI_initial_key = f'phi_{country}_{unique_years[KK]}'\n",
    "            short_var_name_key = f'variable_names_{country}_{unique_years[KK]}'\n",
    "\n",
    "            PHI_initial = PHI_NORMALIZED.get(PHI_initial_key, None)\n",
    "            short_var_names = list_short_names.get(short_var_name_key, [])\n",
    "\n",
    "            # Filter PHI_initial based on the intersection of vars and short_var_names\n",
    "            if PHI_initial is not None:\n",
    "                # Find indices of vars in short_var_names\n",
    "                indices = [short_var_names.index(var) for var in vars if var in short_var_names]\n",
    "\n",
    "                # Filter PHI_initial using numpy's advanced indexing\n",
    "                PHI_initial_filtered = PHI_initial[np.ix_(indices, indices)]\n",
    "                \n",
    "                    # Calculate changes for each metric between initial and final observations\n",
    "            change_MPI = TT_int.loc[zz_final, 'MPI'].values - TT_int.loc[zz_initial, 'MPI'].values\n",
    "            change_A = TT_int.loc[zz_final, 'A_int'].values - TT_int.loc[zz_initial, 'A_int'].values\n",
    "\n",
    "\n",
    "            # Calculate the number of years between the initial and final observations\n",
    "            # Assuming unique_yyrs is a numpy array or a pandas Series of unique years present in TT_int\n",
    "            change_years = unique_years[KK+1] - unique_years[KK]\n",
    "            \n",
    "\n",
    "            # Normalize the changes by the number of years\n",
    "            # These operations assume change_years is a scalar. If change_years is an array, adjustments might be needed\n",
    "            change_A /= change_years\n",
    "\n",
    "\n",
    "            # Prepare the DataFrame for regression analysis by selecting the initial observation\n",
    "            TT_int_for_regression = TT_int.loc[zz_initial, :].copy()\n",
    "\n",
    "            # Add calculated changes to the DataFrame\n",
    "            TT_int_for_regression['change_MPI'] = change_MPI\n",
    "            TT_int_for_regression['change_A'] = change_A\n",
    "            TT_int_for_regression['change_years'] = change_years\n",
    "            TT_int_for_regression['period'] = KK            \n",
    "\n",
    "            # Append the prepared DataFrame to the TT_regression DataFrame\n",
    "            TT_regression_3_period = pd.concat([TT_regression_3_period, TT_int_for_regression], ignore_index=True)\n",
    "            \n",
    "            \n",
    "print('FINISHED')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c08fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>MPI</th>\n",
       "      <th>var_sample</th>\n",
       "      <th>A_int</th>\n",
       "      <th>A_avg</th>\n",
       "      <th>A_avg_raw</th>\n",
       "      <th>A_int_rank</th>\n",
       "      <th>centrality_w_norm</th>\n",
       "      <th>centrality_w_norm_rank</th>\n",
       "      <th>full_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_cm_01</td>\n",
       "      <td>0.005434</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.376074</td>\n",
       "      <td>10</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>10</td>\n",
       "      <td>Child mortality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_nutr_01</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>0.142377</td>\n",
       "      <td>0.347675</td>\n",
       "      <td>5</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>6</td>\n",
       "      <td>Nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_satt_01</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>0.064793</td>\n",
       "      <td>0.368351</td>\n",
       "      <td>8</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>9</td>\n",
       "      <td>School attendance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_educ_01</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>0.369037</td>\n",
       "      <td>9</td>\n",
       "      <td>0.039208</td>\n",
       "      <td>8</td>\n",
       "      <td>Schooling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nam</td>\n",
       "      <td>07</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>d_elct_01</td>\n",
       "      <td>0.062034</td>\n",
       "      <td>0.273219</td>\n",
       "      <td>0.319473</td>\n",
       "      <td>3</td>\n",
       "      <td>0.160981</td>\n",
       "      <td>3</td>\n",
       "      <td>Electricity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_wtr_01</td>\n",
       "      <td>0.062204</td>\n",
       "      <td>0.347358</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>5</td>\n",
       "      <td>0.114861</td>\n",
       "      <td>5</td>\n",
       "      <td>Drinking water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_sani_01</td>\n",
       "      <td>0.093881</td>\n",
       "      <td>0.450380</td>\n",
       "      <td>0.462862</td>\n",
       "      <td>2</td>\n",
       "      <td>0.151528</td>\n",
       "      <td>2</td>\n",
       "      <td>Sanitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_hsg_01</td>\n",
       "      <td>0.074373</td>\n",
       "      <td>0.401509</td>\n",
       "      <td>0.482370</td>\n",
       "      <td>3</td>\n",
       "      <td>0.133203</td>\n",
       "      <td>3</td>\n",
       "      <td>Housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_ckfl_01</td>\n",
       "      <td>0.099045</td>\n",
       "      <td>0.456024</td>\n",
       "      <td>0.457698</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154431</td>\n",
       "      <td>1</td>\n",
       "      <td>Cooking fuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>mdg</td>\n",
       "      <td>18</td>\n",
       "      <td>0.556743</td>\n",
       "      <td>d_asst_01</td>\n",
       "      <td>0.054067</td>\n",
       "      <td>0.320666</td>\n",
       "      <td>0.502676</td>\n",
       "      <td>6</td>\n",
       "      <td>0.105662</td>\n",
       "      <td>6</td>\n",
       "      <td>Assets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1472 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country year       MPI var_sample     A_int     A_avg  A_avg_raw  \\\n",
       "0        nam   07  0.381508    d_cm_01  0.005434  0.024324   0.376074   \n",
       "1        nam   07  0.381508  d_nutr_01  0.033832  0.142377   0.347675   \n",
       "2        nam   07  0.381508  d_satt_01  0.013156  0.064793   0.368351   \n",
       "3        nam   07  0.381508  d_educ_01  0.012470  0.066746   0.369037   \n",
       "4        nam   07  0.381508  d_elct_01  0.062034  0.273219   0.319473   \n",
       "...      ...  ...       ...        ...       ...       ...        ...   \n",
       "1467     mdg   18  0.556743   d_wtr_01  0.062204  0.347358   0.494539   \n",
       "1468     mdg   18  0.556743  d_sani_01  0.093881  0.450380   0.462862   \n",
       "1469     mdg   18  0.556743   d_hsg_01  0.074373  0.401509   0.482370   \n",
       "1470     mdg   18  0.556743  d_ckfl_01  0.099045  0.456024   0.457698   \n",
       "1471     mdg   18  0.556743  d_asst_01  0.054067  0.320666   0.502676   \n",
       "\n",
       "     A_int_rank  centrality_w_norm centrality_w_norm_rank         full_names  \n",
       "0            10           0.014261                     10    Child mortality  \n",
       "1             5           0.084066                      6          Nutrition  \n",
       "2             8           0.038085                      9  School attendance  \n",
       "3             9           0.039208                      8          Schooling  \n",
       "4             3           0.160981                      3        Electricity  \n",
       "...         ...                ...                    ...                ...  \n",
       "1467          5           0.114861                      5     Drinking water  \n",
       "1468          2           0.151528                      2         Sanitation  \n",
       "1469          3           0.133203                      3            Housing  \n",
       "1470          1           0.154431                      1       Cooking fuel  \n",
       "1471          6           0.105662                      6             Assets  \n",
       "\n",
       "[1472 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PHI_NORMALIZED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ae3d0",
   "metadata": {},
   "source": [
    "## FIGURE 1 A\n",
    "\n",
    "The code below reproduces Figure 1 a (the Poverty Space of Turkmenistan in 2006 and 2019). In these networks, nodes are poverty indicators, and edges between them indicate significant connections. The node size corresponds to its poverty centrality, while its color matches the censored headcount ratio of the indicator.\n",
    "\n",
    "We see that for Turkmenistan, Housing was the most central poverty indicator in 2006 (located in the core), while Electricity was the least central (located in the periphery). This suggests a household in 2006 Turkmenistan, if deprived in any other indicator, was most likely to also lack adequate Housing. Conversely, if a household was deprived in any indicator, it was the least probable to also be deprived in Electricity. Various factors such as geographic location, access to quality services, systemic inequality, and socio-economic policies could contribute to this specific poverty structure. The Poverty Space compiles these factors into a network visualization, revealing the underlying relationships between poverty indicators. By 2019, Child Mortality became the most central indicator in Turkmenistan, signifying dynamic relationships among poverty indicators that adapt in response to socio-economic changes and potential policy interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ca03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT FIGURE 1 A\n",
    "\n",
    "# Kyrgyzstan in 2006\n",
    "ctry_init = 'kgz'\n",
    "yyr_init = '06'\n",
    "\n",
    "\n",
    "PHI_w_norm = PHI_NORMALIZED[f'phi_{ctry_init}_{yyr_init}'].copy()\n",
    "filtered_df = TT_FINAL[(TT_FINAL['year'] == yyr_init) & (TT_FINAL['country'] == ctry_init)]\n",
    "A_int = filtered_df[['A_int']].copy()\n",
    "A_int_flat = filtered_df['A_int'].copy()\n",
    "var_sample = filtered_df[['full_names']].copy()\n",
    "centrality_w_norm = filtered_df[['centrality_w_norm']].copy()\n",
    "centrality_w_norm_flat = filtered_df['centrality_w_norm'].copy()\n",
    "\n",
    "\n",
    "# Normalize and scale A_int values for coloring\n",
    "pos_colors = zscore(A_int.squeeze().to_numpy())\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "pos_colors_scaled = scaler.fit_transform(pos_colors.reshape(-1, 1)).flatten().round().astype(int)\n",
    "\n",
    "# Use a matplotlib colormap similar to MATLAB's parula\n",
    "cmap = plt.cm.get_cmap('viridis', 100)\n",
    "node_colors = np.array([cmap(pos_colors_scaled[k]-1) for k in range(len(pos_colors_scaled))])\n",
    "\n",
    "# Convert to Plotly colorscale\n",
    "plotly_cmap = [[i/99, f'rgb({c[0]*255},{c[1]*255},{c[2]*255})'] for i, c in enumerate(cmap.colors)]\n",
    "\n",
    "# Node labels\n",
    "node_labels = var_sample['full_names'].tolist()\n",
    "node_labels_dict = {i: label for i, label in enumerate(node_labels)}\n",
    "\n",
    "# Graph threshold and construction\n",
    "thresh = np.min(np.max(PHI_w_norm, axis=1))\n",
    "XX = np.copy(PHI_w_norm)\n",
    "XX[XX < thresh] = 0\n",
    "XX = np.round(XX / np.min(XX[XX > 0]))\n",
    "XX = XX + XX.T\n",
    "thresh = np.min(np.max(XX, axis=1))\n",
    "XX = np.copy(XX)\n",
    "XX[XX < thresh] = 0\n",
    "XX = (XX > 0).astype(float)\n",
    "G = nx.from_numpy_matrix(XX)\n",
    "\n",
    "                # Assuming A_int_flat and centrality_w_norm_flat are pandas Series or numpy arrays\n",
    "hover_texts = []\n",
    "for i in range(len(G.nodes())):\n",
    "    label = node_labels_dict[i]\n",
    "    CHR_value = A_int_flat.iloc[i] if isinstance(A_int_flat, pd.Series) else A_int_flat[i]\n",
    "    PC_value = centrality_w_norm_flat.iloc[i] if isinstance(centrality_w_norm_flat, pd.Series) else centrality_w_norm_flat[i]\n",
    "    hover_text = f\"{label}<br>CHR = {CHR_value:.2f}<br>PC = {PC_value:.2f}\"\n",
    "    hover_texts.append(hover_text)\n",
    "\n",
    "# Fix random state for layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.append(x0)\n",
    "    edge_x.append(x1)\n",
    "    edge_x.append(None)\n",
    "    edge_y.append(y0)\n",
    "    edge_y.append(y1)\n",
    "    edge_y.append(None)\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines',\n",
    "    showlegend=False)\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "# Scale node sizes based on centrality\n",
    "scaler = MinMaxScaler(feature_range=(10, 50))  # Adjust the range as needed\n",
    "centrality_scaled = scaler.fit_transform(centrality_w_norm_flat.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    textposition=\"top center\",\n",
    "    text=[node_labels_dict[i] for i in range(len(G.nodes()))],\n",
    "    hovertext=hover_texts,\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale=plotly_cmap,\n",
    "        size=centrality_scaled,  # Use the scaled centrality for node size\n",
    "        color=pos_colors_scaled,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Poverty Intensity',\n",
    "            xanchor='center',\n",
    "            titleside='top',\n",
    "            tickvals=[1, 100],\n",
    "            ticktext=['Low', 'High'],\n",
    "            orientation='h',\n",
    "            x=0.5,\n",
    "            y=-0.1\n",
    "        ),\n",
    "        line_width=2),\n",
    "    showlegend=False)\n",
    "\n",
    "# Create dummy traces for node size legend\n",
    "size_legend = [dict(size=10, label='Low Centrality'),\n",
    "               dict(size=30, label='Medium Centrality'),\n",
    "               dict(size=50, label='High Centrality')]\n",
    "\n",
    "legend_traces = []\n",
    "for item in size_legend:\n",
    "    legend_traces.append(go.Scatter(\n",
    "        x=[None], y=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=item['size'],\n",
    "            color='lightgray',\n",
    "            line=dict(width=1, color='black')\n",
    "        ),\n",
    "        legendgroup='Node Size',\n",
    "        showlegend=True,\n",
    "        name=item['label']\n",
    "    ))\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=[edge_trace, node_trace] + legend_traces)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=800,\n",
    "    title=f'{ctry_init.upper()} {yyr_init}',\n",
    "    titlefont_size=16,\n",
    "    showlegend=True,\n",
    "    hovermode='closest',\n",
    "    paper_bgcolor='white',  # Set background color to white\n",
    "    plot_bgcolor='white',  # Set plot area background color to white\n",
    "    margin=dict(b=80, l=40, r=40, t=40),  # Increase bottom margin to fit the colorbar\n",
    "    annotations=[dict(\n",
    "        text=\"Poverty Intensity\",\n",
    "        showarrow=False,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5,\n",
    "        y=-0.1,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    )],\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),  # Hide x-axis ticks\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)  # Hide y-axis ticks\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import zscore\n",
    "import networkx as nx\n",
    "import os\n",
    "import plotly.io as pio\n",
    "import re  # Import regular expressions\n",
    "\n",
    "\n",
    "for key in PHI_NORMALIZED.keys():\n",
    "    try:\n",
    "        # Extract country and year from the key\n",
    "        match = re.match(r'phi_([a-z]+)_([0-9]+)', key)\n",
    "        if not match:\n",
    "            continue  # Skip if the key format is unexpected\n",
    "        ctry_init, yyr_init = match.groups()\n",
    "                \n",
    "        PHI_w_norm = PHI_NORMALIZED[f'phi_{ctry_init}_{yyr_init}'].copy()\n",
    "        filtered_df = TT_FINAL[(TT_FINAL['year'] == yyr_init) & (TT_FINAL['country'] == ctry_init)]\n",
    "        A_int = filtered_df[['A_int']].copy()\n",
    "        A_int_flat = filtered_df['A_int'].copy()\n",
    "        var_sample = filtered_df[['full_names']].copy()\n",
    "        centrality_w_norm = filtered_df[['centrality_w_norm']].copy()\n",
    "        centrality_w_norm_flat = filtered_df['centrality_w_norm'].copy()\n",
    "\n",
    "        # Normalize and scale A_int values for coloring\n",
    "        pos_colors = zscore(A_int.squeeze().to_numpy())\n",
    "        scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "        pos_colors_scaled = scaler.fit_transform(pos_colors.reshape(-1, 1)).flatten().round().astype(int)\n",
    "\n",
    "        # Use a matplotlib colormap similar to MATLAB's parula\n",
    "        cmap = plt.cm.get_cmap('viridis', 100)\n",
    "        node_colors = np.array([cmap(pos_colors_scaled[k]-1) for k in range(len(pos_colors_scaled))])\n",
    "\n",
    "        # Convert to Plotly colorscale\n",
    "        plotly_cmap = [[i/99, f'rgb({c[0]*255},{c[1]*255},{c[2]*255})'] for i, c in enumerate(cmap.colors)]\n",
    "\n",
    "        # Node labels\n",
    "        node_labels = var_sample['full_names'].tolist()\n",
    "        node_labels_dict = {i: label for i, label in enumerate(node_labels)}\n",
    "\n",
    "        # Graph threshold and construction\n",
    "        thresh = np.min(np.max(PHI_w_norm, axis=1))\n",
    "        XX = np.copy(PHI_w_norm)\n",
    "        XX[XX < thresh] = 0\n",
    "        XX = np.round(XX / np.min(XX[XX > 0]))\n",
    "        XX = XX + XX.T\n",
    "        thresh = np.min(np.max(XX, axis=1))\n",
    "        XX = np.copy(XX)\n",
    "        XX[XX < thresh] = 0\n",
    "        XX = (XX > 0).astype(float)\n",
    "        G = nx.from_numpy_matrix(XX)\n",
    "        \n",
    "        \n",
    "                # Assuming A_int_flat and centrality_w_norm_flat are pandas Series or numpy arrays\n",
    "        hover_texts = []\n",
    "        for i in range(len(G.nodes())):\n",
    "            label = node_labels_dict[i]\n",
    "            CHR_value = A_int_flat.iloc[i] if isinstance(A_int_flat, pd.Series) else A_int_flat[i]\n",
    "            PC_value = centrality_w_norm_flat.iloc[i] if isinstance(centrality_w_norm_flat, pd.Series) else centrality_w_norm_flat[i]\n",
    "            hover_text = f\"{label}<br>CHR = {CHR_value:.2f}<br>PC = {PC_value:.2f}\"\n",
    "            hover_texts.append(hover_text)\n",
    "\n",
    "        # Fix random state for layout\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.append(x0)\n",
    "            edge_x.append(x1)\n",
    "            edge_x.append(None)\n",
    "            edge_y.append(y0)\n",
    "            edge_y.append(y1)\n",
    "            edge_y.append(None)\n",
    "\n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=0.5, color='#888'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines',\n",
    "            showlegend=False)\n",
    "\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        for node in G.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "\n",
    "        # Scale node sizes based on centrality\n",
    "        scaler = MinMaxScaler(feature_range=(10, 50))  # Adjust the range as needed\n",
    "        centrality_scaled = scaler.fit_transform(centrality_w_norm_flat.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        textposition=\"top center\",\n",
    "        text=[node_labels_dict[i] for i in range(len(G.nodes()))],\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale=plotly_cmap,\n",
    "            size=centrality_scaled,  # Use the scaled centrality for node size\n",
    "            color=pos_colors_scaled,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Poverty Intensity',\n",
    "                xanchor='center',\n",
    "                titleside='top',\n",
    "                tickvals=[1, 100],\n",
    "                ticktext=['Low', 'High'],\n",
    "                orientation='h',\n",
    "                x=0.5,\n",
    "                y=-0.1\n",
    "            ),\n",
    "            line_width=2),\n",
    "        showlegend=False)\n",
    "\n",
    "        # Create dummy traces for node size legend\n",
    "        size_legend = [dict(size=10, label='Low Centrality'),\n",
    "                       dict(size=30, label='Medium Centrality'),\n",
    "                       dict(size=50, label='High Centrality')]\n",
    "\n",
    "        legend_traces = []\n",
    "        for item in size_legend:\n",
    "            legend_traces.append(go.Scatter(\n",
    "                x=[None], y=[None],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=item['size'],\n",
    "                    color='lightgray',\n",
    "                    line=dict(width=1, color='black')\n",
    "                ),\n",
    "                legendgroup='Node Size',\n",
    "                showlegend=True,\n",
    "                name=item['label']\n",
    "            ))\n",
    "\n",
    "        # Create figure\n",
    "        fig = go.Figure(data=[edge_trace, node_trace] + legend_traces)\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=500,\n",
    "            width=800,\n",
    "            title=f'{ctry_init.upper()} {yyr_init}',\n",
    "            titlefont_size=16,\n",
    "            showlegend=True,\n",
    "            hovermode='closest',\n",
    "            paper_bgcolor='white',  # Set background color to white\n",
    "            plot_bgcolor='white',  # Set plot area background color to white\n",
    "            margin=dict(b=80, l=40, r=40, t=40),  # Increase bottom margin to fit the colorbar\n",
    "            annotations=[dict(\n",
    "                text=\"Poverty Intensity\",\n",
    "                showarrow=False,\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.5,\n",
    "                y=-0.1,\n",
    "                xanchor='center',\n",
    "                yanchor='bottom'\n",
    "            )],\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),  # Hide x-axis ticks\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)  # Hide y-axis ticks\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Save the figure as HTML\n",
    "        html_filename = f'output/figure_{ctry_init}_{yyr_init}.html'\n",
    "        pio.write_html(fig, file=html_filename, full_html=False, include_plotlyjs='cdn')\n",
    "\n",
    "        # Save matrices as CSV\n",
    "        PHI_w_norm_df = pd.DataFrame(PHI_w_norm, index=node_labels, columns=node_labels)\n",
    "        PHI_w_norm_df.to_csv(f'output/Network_data_weighted_{ctry_init}_{yyr_init}.csv')\n",
    "        XX_df = pd.DataFrame(XX, index=node_labels, columns=node_labels)\n",
    "        XX_df.to_csv(f'output/Network_data_binary_{ctry_init}_{yyr_init}.csv')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ctry_init} {yyr_init}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dash plotly pandas numpy scikit-learn networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory where the HTML files are stored\n",
    "output_dir = 'output'\n",
    "embed_dir = 'embed_codes'\n",
    "os.makedirs(embed_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each HTML file in the output directory\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.endswith(\".html\"):\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Read the content of the HTML file\n",
    "        with open(file_path, 'r') as f:\n",
    "            html_content = f.read()\n",
    "        \n",
    "        # Find the start and end of the <div> and <script> tags for the plot\n",
    "        div_start = html_content.find('<div')\n",
    "        script_start = html_content.find('<script')\n",
    "        script_end = html_content.rfind('</script>') + len('</script>')\n",
    "\n",
    "        # Extract the <div> and <script> sections as the embed code\n",
    "        embed_code = html_content[div_start:script_end]\n",
    "\n",
    "        # Save the embed code to a new text file\n",
    "        embed_filename = os.path.join(embed_dir, filename.replace(\".html\", \"_embed_code.txt\"))\n",
    "        with open(embed_filename, 'w') as embed_file:\n",
    "            embed_file.write(embed_code)\n",
    "\n",
    "        print(f\"Embed code for {filename} saved to {embed_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT_FINAL.to_excel('tables-data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98192329",
   "metadata": {},
   "source": [
    "## FIGURE 1 B\n",
    "\n",
    "The code below reproduces Figure 1 a (the Poverty Space of Ethiopia in 2011 and 2019). In these networks, nodes are poverty indicators, and edges between them indicate significant connections. The node size corresponds to its poverty centrality, while its color matches the censored headcount ratio of the indicator.\n",
    "\n",
    "In Ethiopia, the Poverty Space structure remained largely consistent over time. In both 2011 and 2019, Cooking Fuel was the most central indicator (located in the core), while Child Mortality was the least central (located in the periphery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT FIGURE 1 B\n",
    "\n",
    "\n",
    "# ETHIOPIA in 2011\n",
    "ctry_init = 'eth'\n",
    "yyr_init = '11'\n",
    "\n",
    "\n",
    "PHI_w_norm = PHI_NORMALIZED[f'phi_{ctry_init}_{yyr_init}'].copy()\n",
    "filtered_df = TT_FINAL[(TT_FINAL['year'] == yyr_init) & (TT_FINAL['country'] == ctry_init)]\n",
    "A_int = filtered_df[['A_int']].copy()\n",
    "var_sample = filtered_df[['full_names']].copy()\n",
    "centrality_w_norm = filtered_df[['centrality_w_norm']].copy()\n",
    "\n",
    "# Normalize and scale A_int values for coloring\n",
    "pos_colors = zscore(A_int.squeeze().to_numpy())\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "pos_colors_scaled = scaler.fit_transform(pos_colors.reshape(-1, 1)).flatten().round().astype(int)\n",
    "\n",
    "# Use a matplotlib colormap similar to MATLAB's parula\n",
    "cmap = plt.cm.get_cmap('viridis', 100)\n",
    "node_colors = np.array([cmap(pos_colors_scaled[k]-1) for k in range(len(pos_colors_scaled))])\n",
    "\n",
    "# Convert to Plotly colorscale\n",
    "plotly_cmap = [[i/99, f'rgb({c[0]*255},{c[1]*255},{c[2]*255})'] for i, c in enumerate(cmap.colors)]\n",
    "\n",
    "# Node labels\n",
    "node_labels = var_sample['full_names'].tolist()\n",
    "node_labels_dict = {i: label for i, label in enumerate(node_labels)}\n",
    "\n",
    "# Graph threshold and construction\n",
    "thresh = np.min(np.max(PHI_w_norm, axis=1))\n",
    "XX = np.copy(PHI_w_norm)\n",
    "XX[XX < thresh] = 0\n",
    "XX = np.round(XX / np.min(XX[XX > 0]))\n",
    "XX = XX + XX.T\n",
    "thresh = np.min(np.max(XX, axis=1))\n",
    "XX = np.copy(XX)\n",
    "XX[XX < thresh] = 0\n",
    "XX = (XX > 0).astype(float)\n",
    "G = nx.from_numpy_matrix(XX)\n",
    "\n",
    "# Sort var_sample based on centrality_w_norm for ranking\n",
    "centrality_w_norm_flat = centrality_w_norm.squeeze()\n",
    "A_int_flat = A_int.squeeze()\n",
    "ranked_df = var_sample.assign(A_int=A_int_flat)\n",
    "ranked_df = ranked_df.assign(centrality=centrality_w_norm_flat).sort_values(by='centrality', ascending=False)\n",
    "ranked_df.reset_index(drop=True, inplace=True)\n",
    "ranked_df['Rank'] = ranked_df.index + 1\n",
    "table_df = ranked_df[['Rank', 'full_names','A_int','centrality']]\n",
    "table_df.columns = ['Rank', 'Name','CHR','PC']\n",
    "\n",
    "# Fix random state for layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.append(x0)\n",
    "    edge_x.append(x1)\n",
    "    edge_x.append(None)\n",
    "    edge_y.append(y0)\n",
    "    edge_y.append(y1)\n",
    "    edge_y.append(None)\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "# Scale node sizes based on centrality\n",
    "scaler = MinMaxScaler(feature_range=(10, 50))  # Adjust the range as needed\n",
    "centrality_scaled = scaler.fit_transform(centrality_w_norm_flat.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    textposition=\"top center\",\n",
    "    text=[node_labels_dict[i] for i in range(len(G.nodes()))],\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale=plotly_cmap,\n",
    "        size=centrality_scaled,  # Use the scaled centrality for node size\n",
    "        color=pos_colors_scaled,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Poverty Intensity',\n",
    "            xanchor='center',\n",
    "            titleside='top',\n",
    "            tickvals=[1, 100],\n",
    "            ticktext=['Low', 'High'],\n",
    "            orientation='h',\n",
    "            x=0.5,\n",
    "            y=-0.1\n",
    "        ),\n",
    "        line_width=2))\n",
    "\n",
    "# Adjusting layout to separate the graph and the table\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    column_widths=[0.75, 0.25],  # Set the relative widths of the columns\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"table\"}]]\n",
    ")\n",
    "\n",
    "# Add network graph\n",
    "fig.add_trace(edge_trace, row=1, col=1)\n",
    "fig.add_trace(node_trace, row=1, col=1)\n",
    "\n",
    "# Add table\n",
    "table_trace = go.Table(\n",
    "    header=dict(\n",
    "        values=['PC Rank', 'Indicator','CHR','PC'],\n",
    "        fill_color='paleturquoise',\n",
    "        align='left'\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[table_df.Rank, table_df.Name,np.round(table_df.CHR,4),np.round(table_df.PC,4)],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(table_trace, row=1, col=2)\n",
    "\n",
    "# Update layout to separate graph and table, with white background and no x/y axis ticks\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    title='ETHIOPIA 2011',\n",
    "    titlefont_size=16,\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    paper_bgcolor='white',  # Set background color to white\n",
    "    plot_bgcolor='white',  # Set plot area background color to white\n",
    "    margin=dict(b=80, l=40, r=40, t=40),  # Increase bottom margin to fit the colorbar\n",
    "    annotations=[dict(\n",
    "        text=\"Poverty Intensity\",\n",
    "        showarrow=False,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5,\n",
    "        y=-0.1,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    )],\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),  # Hide x-axis ticks\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)  # Hide y-axis ticks\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "pio.write_image(fig, 'output/figure1b_eth11.pdf', format='pdf', width=1200, height=800, scale=2)\n",
    "\n",
    "# ETHIOPIA in 2019\n",
    "ctry_init = 'eth'\n",
    "yyr_init = '19'\n",
    "\n",
    "PHI_w_norm = PHI_NORMALIZED[f'phi_{ctry_init}_{yyr_init}'].copy()\n",
    "filtered_df = TT_FINAL[(TT_FINAL['year'] == yyr_init) & (TT_FINAL['country'] == ctry_init)]\n",
    "A_int = filtered_df[['A_int']].copy()\n",
    "var_sample = filtered_df[['full_names']].copy()\n",
    "centrality_w_norm = filtered_df[['centrality_w_norm']].copy()\n",
    "\n",
    "# Normalize and scale A_int values for coloring\n",
    "pos_colors = zscore(A_int.squeeze().to_numpy())\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "pos_colors_scaled = scaler.fit_transform(pos_colors.reshape(-1, 1)).flatten().round().astype(int)\n",
    "\n",
    "# Use a matplotlib colormap similar to MATLAB's parula\n",
    "cmap = plt.cm.get_cmap('viridis', 100)\n",
    "node_colors = np.array([cmap(pos_colors_scaled[k]-1) for k in range(len(pos_colors_scaled))])\n",
    "\n",
    "# Convert to Plotly colorscale\n",
    "plotly_cmap = [[i/99, f'rgb({c[0]*255},{c[1]*255},{c[2]*255})'] for i, c in enumerate(cmap.colors)]\n",
    "\n",
    "# Node labels\n",
    "node_labels = var_sample['full_names'].tolist()\n",
    "node_labels_dict = {i: label for i, label in enumerate(node_labels)}\n",
    "\n",
    "# Graph threshold and construction\n",
    "thresh = np.min(np.max(PHI_w_norm, axis=1))\n",
    "XX = np.copy(PHI_w_norm)\n",
    "XX[XX < thresh] = 0\n",
    "XX = np.round(XX / np.min(XX[XX > 0]))\n",
    "XX = XX + XX.T\n",
    "thresh = np.min(np.max(XX, axis=1))\n",
    "XX = np.copy(XX)\n",
    "XX[XX < thresh] = 0\n",
    "XX = (XX > 0).astype(float)\n",
    "G = nx.from_numpy_matrix(XX)\n",
    "\n",
    "# Sort var_sample based on centrality_w_norm for ranking\n",
    "centrality_w_norm_flat = centrality_w_norm.squeeze()\n",
    "A_int_flat = A_int.squeeze()\n",
    "ranked_df = var_sample.assign(A_int=A_int_flat)\n",
    "ranked_df = ranked_df.assign(centrality=centrality_w_norm_flat).sort_values(by='centrality', ascending=False)\n",
    "ranked_df.reset_index(drop=True, inplace=True)\n",
    "ranked_df['Rank'] = ranked_df.index + 1\n",
    "table_df = ranked_df[['Rank', 'full_names','A_int','centrality']]\n",
    "table_df.columns = ['Rank', 'Name','CHR','PC']\n",
    "\n",
    "# Fix random state for layout\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.append(x0)\n",
    "    edge_x.append(x1)\n",
    "    edge_x.append(None)\n",
    "    edge_y.append(y0)\n",
    "    edge_y.append(y1)\n",
    "    edge_y.append(None)\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "# Scale node sizes based on centrality\n",
    "scaler = MinMaxScaler(feature_range=(10, 50))  # Adjust the range as needed\n",
    "centrality_scaled = scaler.fit_transform(centrality_w_norm_flat.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    textposition=\"top center\",\n",
    "    text=[node_labels_dict[i] for i in range(len(G.nodes()))],\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale=plotly_cmap,\n",
    "        size=centrality_scaled,  # Use the scaled centrality for node size\n",
    "        color=pos_colors_scaled,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Poverty Intensity',\n",
    "            xanchor='center',\n",
    "            titleside='top',\n",
    "            tickvals=[1, 100],\n",
    "            ticktext=['Low', 'High'],\n",
    "            orientation='h',\n",
    "            x=0.5,\n",
    "            y=-0.1\n",
    "        ),\n",
    "        line_width=2))\n",
    "\n",
    "# Adjusting layout to separate the graph and the table\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    column_widths=[0.75, 0.25],  # Set the relative widths of the columns\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"table\"}]]\n",
    ")\n",
    "\n",
    "# Add network graph\n",
    "fig.add_trace(edge_trace, row=1, col=1)\n",
    "fig.add_trace(node_trace, row=1, col=1)\n",
    "\n",
    "# Add table\n",
    "table_trace = go.Table(\n",
    "    header=dict(\n",
    "        values=['PC Rank', 'Indicator','CHR','PC'],\n",
    "        fill_color='paleturquoise',\n",
    "        align='left'\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[table_df.Rank, table_df.Name,np.round(table_df.CHR,4),np.round(table_df.PC,4)],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(table_trace, row=1, col=2)\n",
    "\n",
    "# Update layout to separate graph and table, with white background and no x/y axis ticks\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    title='ETHIOPIA 2019',\n",
    "    titlefont_size=16,\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    paper_bgcolor='white',  # Set background color to white\n",
    "    plot_bgcolor='white',  # Set plot area background color to white\n",
    "    margin=dict(b=80, l=40, r=40, t=40),  # Increase bottom margin to fit the colorbar\n",
    "    annotations=[dict(\n",
    "        text=\"Poverty Intensity\",\n",
    "        showarrow=False,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5,\n",
    "        y=-0.1,\n",
    "        xanchor='center',\n",
    "        yanchor='bottom'\n",
    "    )],\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),  # Hide x-axis ticks\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)  # Hide y-axis ticks\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "pio.write_image(fig, 'output/figure1b_eth19.pdf', format='pdf', width=1200, height=800, scale=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b240da",
   "metadata": {},
   "source": [
    "## FIGURE 2 A\n",
    "\n",
    "In Figure 2 a, we investigate the relationship between the poverty space and the censored headcount ratio, by pooling data from every country and every period. We find a correlation between poverty centrality and the headcount ratio . Though, this correlation is only moderate, suggesting that the poverty space and the headcount ratio do not offer the same information about the structure of multidimensional poverty.\n",
    "\n",
    "## FIGURE 2 B\n",
    "\n",
    "In Figure 2 b, we determined the median centrality for each indicator across all countries, using only data from the final survey year. This measurement provides a snapshot of an indicator's \"typical\" importance across countries, offering a benchmark to assess the uniqueness of a country's poverty structure. On average, Cooking Fuel emerges as the most central indicator (located in the core), with Child Mortality being the least central (located in the periphery). \n",
    "\n",
    "## FIGURE 2 C\n",
    "\n",
    "We evaluated the sensitivity of the Poverty Space in each country to missing poverty indicators by removing one poverty indicator at a time, recalculating the structural dependence matrix, and comparing the Poverty Centrality of the rest of the indicators in the recalculated matrix to their original values. Figure 2 c displays boxplots representing the distribution of the correlation between the poverty centrality measures when one indicator is removed. Our analysis reveals that this correlation is almost always above 0.9. This lack of significant differences suggests the robustness of the Poverty Space.\n",
    "\n",
    "## FIGURE 2 D\n",
    "\n",
    "Bar chart for each country giving the correlations between the centrality of an indicator in the initial and final survey year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed04e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2A\n",
    "correlation_coef = np.corrcoef(TT_FINAL['centrality_w_norm'], TT_FINAL['A_int'])[0, 1]\n",
    "slope, intercept, r_value, p_value, std_err = linregress(TT_FINAL['centrality_w_norm'], TT_FINAL['A_int'])\n",
    "regression_line = slope * TT_FINAL['centrality_w_norm'] + intercept\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3, subplot_titles=('a', 'b', 'c', 'd'), horizontal_spacing=0.08, vertical_spacing=0.15,\n",
    "                    specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "                           [{\"colspan\": 3}, None, None]])\n",
    "\n",
    "# Scatter plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=TT_FINAL['centrality_w_norm'],\n",
    "    y=TT_FINAL['A_int'],\n",
    "    mode='markers',\n",
    "    marker=dict(color='rgba(31, 119, 180, 0.8)', size=8, line=dict(color='DarkSlateGrey', width=1)),\n",
    "    name='Data Points'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Regression line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=TT_FINAL['centrality_w_norm'],\n",
    "    y=regression_line,\n",
    "    mode='lines',\n",
    "    line=dict(color='rgba(255, 0, 0, 0.8)', width=2, dash='dash'),\n",
    "    name='Regression Line'\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text='Poverty Centrality', showline=True, linewidth=2, linecolor='black', row=1, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(title_text='Censored Headcount Ratio', showline=True, linewidth=2, linecolor='black', row=1, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.01, y=0.98, xref='paper', yref='paper',\n",
    "    text=f'Correlation Coefficient: {correlation_coef:.2f}',\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color='black'),\n",
    "    bgcolor='white',\n",
    "    opacity=1\n",
    ")\n",
    "\n",
    "# Figure 2B\n",
    "median_indicator = TT_regression.groupby('full_names')['centrality_w_norm_final'].median().reset_index()\n",
    "median_indicator_sorted_general = median_indicator.sort_values(by='centrality_w_norm_final')\n",
    "\n",
    "# Horizontal bar plot\n",
    "fig.add_trace(go.Bar(\n",
    "    x=median_indicator_sorted_general['centrality_w_norm_final'],\n",
    "    y=median_indicator_sorted_general['full_names'],\n",
    "    orientation='h',\n",
    "    marker=dict(color='rgba(31, 119, 180, 0.8)', line=dict(color='DarkSlateGrey', width=1))\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Median Poverty Centrality', showline=True, linewidth=2, linecolor='black', row=1, col=2,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, row=1, col=2,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "# Figure 2C\n",
    "sorted_full_variable_names = median_indicator_sorted_general['full_names'].tolist()\n",
    "sorted_indices = [full_variable_names.index(name) for name in sorted_full_variable_names]\n",
    "\n",
    "for i in range(len(sorted_indices)):\n",
    "    column_data = Correlation_matrix[:, sorted_indices[i]]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=column_data[~np.isnan(column_data)],\n",
    "        name=sorted_full_variable_names[i],\n",
    "        orientation='h',\n",
    "        boxmean='sd',\n",
    "        marker_color='rgba(31, 119, 180, 0.8)'\n",
    "    ), row=1, col=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=1000,\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Correlation Coefficient', showline=True, linewidth=2, linecolor='black', row=1, col=3,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(showticklabels=False, row=1, col=3)  # Hide y-axis labels for 2C\n",
    "\n",
    "# Align y-axis labels for 2B and 2C\n",
    "fig.update_yaxes(tickmode='array', tickvals=list(range(len(sorted_full_variable_names))), ticktext=sorted_full_variable_names, row=1, col=2,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "# Figure 2D\n",
    "unique_countries = TT_regression['country'].unique()\n",
    "\n",
    "# Initialize a list to store correlation values for each country\n",
    "corr_indicators = []\n",
    "\n",
    "# Calculate the correlation for 'centrality_w_norm' and 'centrality_w_norm_final' for each country\n",
    "for country in unique_countries:\n",
    "    # Filter the DataFrame for the current country\n",
    "    filtered_df = TT_regression[TT_regression['country'] == country]\n",
    "    \n",
    "    # Calculate correlation and append to list\n",
    "    correlation = filtered_df['centrality_w_norm'].corr(filtered_df['centrality_w_norm_final'])\n",
    "    corr_indicators.append(correlation)\n",
    "\n",
    "# Convert the list of correlations to a NumPy array for sorting\n",
    "corr_indicators = np.array(corr_indicators)\n",
    "\n",
    "# Sort the correlations and get the sorted indices\n",
    "sorted_indices = np.argsort(corr_indicators)\n",
    "\n",
    "# Vertical bar plot for correlations\n",
    "fig.add_trace(go.Bar(\n",
    "    x=np.array(unique_countries)[sorted_indices],\n",
    "    y=corr_indicators[sorted_indices],\n",
    "    marker=dict(color='rgba(31, 119, 180, 0.8)', line=dict(color='DarkSlateGrey', width=1))\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text='Country', showline=True, linewidth=2, linecolor='black', row=2, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(title_text='Poverty centrality correlation in initial and final years of survey', showline=True, linewidth=2, linecolor='black', row=2, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "fig.update_layout(height=800)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Export the figure as a high-quality PDF\n",
    "pio.write_image(fig, 'output/figure2.pdf', format='pdf', width=1200, height=800, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d06a",
   "metadata": {},
   "source": [
    "## TABLE 1\n",
    "\n",
    "We explore the empirical association between the Poverty Centrality and the long run changes in the censored headcount ratio of a poverty indicator by pooling data from all participating countries and constructing regression models in which the dependent variable is the change in the censored headcount ratio from the first to the last year of surveys for each poverty indicator and country. In these regressions our Poverty Centrality is used as an explanatory variable.\n",
    "\n",
    "\n",
    "In column (1) of Table 1, we present a baseline regression model that includes only the dummy variables, which accounts for approximately 29% of the observed changes in the censored headcount ratio (Adjusted R^2=0.29). When we our poverty centrality index (column (2)), we get a significant improvement in the model's explanatory power (Adjusted R^2=0.37). More importantly, we find that more central indicators usually have larger decreases in the headcount ratio, as suggested by our analysis in the previous section.\n",
    "\n",
    "In column (3), we include the initial censored headcount ratio of the indicator H_ci (t) as an additional variable, while column (4) adds the sum of the headcount ratios of all other indicators, excluding the one under consideration. Lastly, in column (5), we include both control variables in our regression model. Our results demonstrate that the negative and significant relationship between poverty centrality and long-run changes in headcount ratios persists even after adjusting for these controls. Additionally, we find that indicators with higher initial censored headcount ratios display larger decreases over time. The sum of the headcount ratios of also has a negative coefficient in the final model, though it is not statistically significant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b90116",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE 1\n",
    "\n",
    "# OLS regression examples\n",
    "reg_baseline = smf.ols(formula='change_A ~ var_sample + country', data=TT_regression).fit(cov_type='HC1')\n",
    "reg_centrality = smf.ols(formula='change_A ~ centrality_w_norm + var_sample + country', data=TT_regression).fit(cov_type='HC1')\n",
    "reg_addiitonal_exp_A_int = smf.ols(formula='change_A ~ A_int + centrality_w_norm + var_sample + country', data=TT_regression).fit(cov_type='HC1')\n",
    "reg_addiitonal_exp_A_avg = smf.ols(formula='change_A ~ A_avg_raw + centrality_w_norm + var_sample + country', data=TT_regression).fit(cov_type='HC1')\n",
    "reg_addiitonal_exp = smf.ols(formula='change_A ~ A_int + A_avg_raw + centrality_w_norm + var_sample + country', data=TT_regression).fit(cov_type='HC1')\n",
    "\n",
    "\n",
    "# Display regression results using stargazer\n",
    "stargazer = Stargazer([reg_baseline, reg_centrality, reg_addiitonal_exp_A_int, reg_addiitonal_exp_A_avg, reg_addiitonal_exp])\n",
    "\n",
    "# Customize stargazer output\n",
    "stargazer.title('Change in headcount ratio models results')\n",
    "stargazer.custom_columns(['(1)', '(2)', '(3)', '(4)', '(5)'], [1, 1, 1, 1, 1])\n",
    "stargazer.significant_digits(3)\n",
    "stargazer.show_model_numbers(False)\n",
    "stargazer.dependent_variable_name('Change in headcount ratio per year')\n",
    "\n",
    "# Customize the order of the variables\n",
    "stargazer.covariate_order([\n",
    "    'centrality_w_norm', \n",
    "    'A_int', \n",
    "    'A_avg_raw'\n",
    "])\n",
    "\n",
    "stargazer.rename_covariates({\n",
    "    'A_int': 'Initial censored headcount ratio',\n",
    "    'centrality_w_norm': 'Initial poverty centrality',\n",
    "    'A_avg_raw': 'Initial headcount ratios of all other indicators',\n",
    "})\n",
    "\n",
    "# Render the HTML and save it to a file\n",
    "html_output = stargazer.render_html()\n",
    "\n",
    "# Save the HTML output to a file\n",
    "with open('output/table1.html', 'w') as file:\n",
    "    file.write(html_output)\n",
    "\n",
    "# Display the HTML in Jupyter Notebook\n",
    "display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ced4f",
   "metadata": {},
   "source": [
    "## TABLE 2\n",
    "\n",
    "In Table 2, we utilize data for 15 countries that have an additional survey conducted between the initial and final years of the survey period. For these nations, we estimate the Poverty Space (and the corresponding Poverty Centrality) for all three survey years and compute the change in headcount ratio between the first and second, as well as the second and third survey years. This enables us to construct a new unbalanced sample which is then used to re-estimate our model. To maintain the restrictiveness of the model we also include year dummies in this analysis. The results, affirm that Poverty Centrality remains a significant and negative predictor of long-run changes in headcount ratios even within this more homogeneous sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9268401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE 2\n",
    "\n",
    "# OLS regression examples\n",
    "reg_baseline = smf.ols(formula='change_A ~ var_sample + country + C(year)', data=TT_regression_3_period).fit(cov_type='HC1')\n",
    "reg_centrality = smf.ols(formula='change_A ~ centrality_w_norm + var_sample + country + C(year)', data=TT_regression_3_period).fit(cov_type='HC1')\n",
    "reg_addiitonal_exp_A_int = smf.ols(formula='change_A ~ A_int + centrality_w_norm + var_sample + country + C(year)', data=TT_regression_3_period).fit(cov_type='HC1')\n",
    "reg_addiitonal_exp_A_avg = smf.ols(formula='change_A ~ A_avg_raw + centrality_w_norm + var_sample + country + C(year)', data=TT_regression_3_period).fit(cov_type='HC1')\n",
    "reg_addiitonal_exp = smf.ols(formula='change_A ~ A_int + A_avg_raw + centrality_w_norm + var_sample + country + C(year)', data=TT_regression_3_period).fit(cov_type='HC1')\n",
    "\n",
    "\n",
    "# Display regression results using stargazer\n",
    "stargazer = Stargazer([reg_baseline, reg_centrality, reg_addiitonal_exp_A_int, reg_addiitonal_exp_A_avg, reg_addiitonal_exp])\n",
    "\n",
    "# Customize stargazer output\n",
    "stargazer.title('Change in headcount ratio models results')\n",
    "stargazer.custom_columns(['Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5'], [1, 1, 1, 1, 1])\n",
    "stargazer.significant_digits(3)\n",
    "stargazer.show_model_numbers(False)\n",
    "stargazer.dependent_variable_name('Change in headcount ratio per year')\n",
    "\n",
    "# Customize the order of the variables\n",
    "stargazer.covariate_order([\n",
    "    'centrality_w_norm', \n",
    "    'A_int', \n",
    "    'A_avg_raw'\n",
    "])\n",
    "\n",
    "stargazer.rename_covariates({\n",
    "    'A_int': 'Initial censored headcount ratio',\n",
    "    'centrality_w_norm': 'Initial poverty centrality',\n",
    "    'A_avg_raw': 'Initial headcount ratios of all other indicators',\n",
    "})\n",
    "\n",
    "# Render the HTML and save it to a file\n",
    "html_output = stargazer.render_html()\n",
    "\n",
    "# Save the HTML output to a file\n",
    "with open('output/table2.html', 'w') as file:\n",
    "    file.write(html_output)\n",
    "\n",
    "# Display the HTML in Jupyter Notebook\n",
    "display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891c728",
   "metadata": {},
   "source": [
    "## TABLE 3\n",
    "\n",
    "Table 3, presents additional robustness results where we use an IV approach to estimate the relationship between poverty centrality and future changes in the censored headcount ratio. Endogeneity might because of unobserved factors influencing both Poverty Centrality and the changes in the censored headcount ratio. This scenario could lead to biased coefficient estimates.\n",
    "\n",
    "The IV approach helps us tackle this issue by using an additional variable as an instrument. This variable should be related with the Poverty centrality, but uncorrelated with the error term of our model. Here, for each indicator in a country we define the instrumental variable as the respective poverty centrality of the same indicator in the country with the most similar poverty space in the initial time period (estimate through the coefficient of correlation between the edges of the Poverty Space). The logic behind this instrument is that countries with similar Poverty Spaces should also exhibit similar Poverty Centrality, but the exact circumstances (such as policy environment, demographic dynamics, etc.) leading to changes in the censored headcount ratio will not necessarily be the same, providing us with a source of exogenous variation.\n",
    "\n",
    "\n",
    "The results consistently indicate that Poverty Centrality is a significant and negative predictor of the long-run changes in the headcount ratio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "TT_regression_cleaned = TT_regression.dropna(subset=['centrality_w_norm_inst'])\n",
    "\n",
    "# OLS regression examples\n",
    "reg_baseline = smf.ols(formula='change_A ~ var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "\n",
    "reg_1_stage_centrality = smf.ols(formula='centrality_w_norm ~ centrality_w_norm_inst + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "TT_regression_cleaned['centrality_w_norm_predicted'] = reg_1_stage_centrality.predict(TT_regression_cleaned)\n",
    "reg_centrality = smf.ols(formula='change_A ~ centrality_w_norm_predicted + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "\n",
    "reg_1_stage_addiitonal_exp_A_int = smf.ols(formula='centrality_w_norm ~ A_int + centrality_w_norm_inst + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "TT_regression_cleaned['centrality_w_norm_predicted'] = reg_1_stage_addiitonal_exp_A_int.predict(TT_regression_cleaned)\n",
    "reg_addiitonal_exp_A_int = smf.ols(formula='change_A ~ A_int + centrality_w_norm_predicted + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "\n",
    "reg_1_stage_addiitonal_exp_A_avg = smf.ols(formula='centrality_w_norm ~ A_avg_raw + centrality_w_norm_inst + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "TT_regression_cleaned['centrality_w_norm_predicted'] = reg_1_stage_addiitonal_exp_A_avg.predict(TT_regression_cleaned)\n",
    "reg_addiitonal_exp_A_avg = smf.ols(formula='change_A ~ A_avg_raw + centrality_w_norm_predicted + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "\n",
    "reg_1_stage_addiitonal_exp = smf.ols(formula='centrality_w_norm ~ A_int + A_avg_raw + centrality_w_norm_inst + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "TT_regression_cleaned['centrality_w_norm_predicted'] = reg_1_stage_addiitonal_exp.predict(TT_regression_cleaned)\n",
    "reg_addiitonal_exp = smf.ols(formula='change_A ~ A_int + A_avg_raw + centrality_w_norm_predicted + var_sample + country', data=TT_regression_cleaned).fit(cov_type='HC1')\n",
    "\n",
    "# Display regression results using stargazer\n",
    "stargazer = Stargazer([reg_baseline, reg_centrality, reg_addiitonal_exp_A_int, reg_addiitonal_exp_A_avg, reg_addiitonal_exp])\n",
    "\n",
    "# Customize stargazer output\n",
    "stargazer.title('Change in headcount ratio models results')\n",
    "stargazer.custom_columns(['Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5'], [1, 1, 1, 1, 1])\n",
    "stargazer.significant_digits(3)\n",
    "stargazer.show_model_numbers(False)\n",
    "stargazer.dependent_variable_name('Change in headcount ratio per year')\n",
    "\n",
    "# Customize the order of the variables\n",
    "stargazer.covariate_order([\n",
    "    'centrality_w_norm_predicted', \n",
    "    'A_int', \n",
    "    'A_avg_raw'\n",
    "])\n",
    "\n",
    "stargazer.rename_covariates({\n",
    "    'A_int': 'Initial censored headcount ratio',\n",
    "    'centrality_w_norm_predicted': 'Initial poverty centrality',\n",
    "    'A_avg_raw': 'Initial headcount ratios of all other indicators',\n",
    "})\n",
    "\n",
    "# Calculate first-stage F-statistics\n",
    "first_stage_models = [reg_1_stage_centrality, reg_1_stage_addiitonal_exp_A_int, reg_1_stage_addiitonal_exp_A_avg, reg_1_stage_addiitonal_exp]\n",
    "f_statistics = []\n",
    "for model in first_stage_models:\n",
    "    f_test = model.f_test(\"centrality_w_norm_inst = 0\")\n",
    "    f_statistics.append(f_test.fvalue)  # Handle as float directly\n",
    "\n",
    "# Render the HTML\n",
    "html_output = stargazer.render_html()\n",
    "\n",
    "# Add F-statistics row to the HTML\n",
    "html_output = html_output.replace(\n",
    "    '</table>',\n",
    "    '<tr><td>First-stage F-statistic</td><td></td>' +\n",
    "    ''.join(f'<td>{f_stat:.3f}</td>' for f_stat in f_statistics) +\n",
    "    '</tr></table>'\n",
    ")\n",
    "\n",
    "# Save the HTML output to a file\n",
    "with open('output/table3.html', 'w') as file:\n",
    "    file.write(html_output)\n",
    "\n",
    "# Display the HTML in Jupyter Notebook\n",
    "display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a869ac8",
   "metadata": {},
   "source": [
    "#  PPI ANALYSIS\n",
    "# (Figure 3)\n",
    "\n",
    "In the empirical analysis we found that the Poverty Space is predictive for the future changes in the censored headcount ratio of a poverty indicator.\n",
    "\n",
    "Motivated by this, here we use the Poverty Space in the Policy Priority Inference (PPI) tool. The\n",
    "PPI is a computational framework created to understand the complexity of policy prioritization and to support governments who wish to distribute transformative resources across numerous policy issues with the aim of reaching specific development goals.\n",
    "\n",
    "As an example, we use Ethiopia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d54c9c",
   "metadata": {},
   "source": [
    "### Model initialization\n",
    "\n",
    "PPI offers two distinct methods for analyzing multidimensional poverty. First, the “retrospective” analysis seeks to understand past dynamics by leveraging historical data from an initial survey year, revealing how poverty indicators and policy priorities (budget allocations for an indicator) have evolved over a certain observation period to reach observed values in a final year. This analysis can infer an optimal spillover rate (the spillover rate that best fits the data) and other model parameters (e.g., α_ci and α_ci^'). Understanding these parameters is essential as spillover rates can vary widely or modestly between dimensions and countries, influencing policy intervention outcomes. Second, the “prospective” analysis uses historical insights to understand how changes in the spillover rate or the Poverty Space could affect multidimensional poverty dynamics over a future period. This approach guides policymakers on which poverty indicators to prioritize and when, helping to inform effective intervention strategies for future improvements.\n",
    "\n",
    "In the retrospective analysis, we use national data on poverty indicators from the first survey year (2011) and the last survey year (2019), government expenditure data (per capita in constant USD) from 2011 to 2019, data on the quality of law and monitoring from the World Bank’s World Governance Indicators for 2011 (the quality of law is approximated with the rule of law variable, whereas the quality of monitoring with the control of corruption), and the estimated Poverty Space for 2011. We estimate the model parameters and find the optimal spillover rate by calibrating the model with spillover rates ranging from 0 to 1 (in increments of 0.05) and calculating the goodness of fit. The rate providing the best fit on average is our optimal spillover rate. See Appendix 4 for more details on the data cleaning and calibration procedures for our PPI analysis.\n",
    "\n",
    "We then conduct a prospective analysis until 2030 using data from the final survey year (2019) and Poverty Space data for the same year. We assume that the rule of law and quality of monitoring remain at the 2019 level and that government expenditure remains at its average value from 2011 to 2019. In this analysis, we again vary the spillover rate from 0 to 1 to perturb the Poverty Space structure and assess how poverty interlinkages could impact multidimensional poverty dynamics.\n",
    "\n",
    "### Findings: \n",
    "\n",
    "Figure 3 details the impact of the Poverty Space on the inferred PPI simulations.\n",
    "\n",
    "First, in Figure 3a we visualize how varying the spillover rate impacts PPI’s estimated MPI for Ethiopia in 2030 (black line). The blue horizontal dashed line represents Ethiopia’s MPI in 2019 (0.567, the beginning of the prospective simulation), while the red vertical line marks the optimal spillover rate identified in the retrospective analysis. Our analysis finds this value to be 0.7, indicating that spillovers could heavily influence Ethiopia’s multidimensional poverty dynamics. If the spillover rate remains at this level, we project Ethiopia’s MPI to decrease by 0.103 units by 2030 (to 0.464). Conversely, if the spillover rate drops to 0, the MPI would fall by only 0.036 units (to 0.531). An increase in the spillover rate to 1 (implying that the spillover network matches the Poverty Space) would result in a 0.109 unit decline in MPI (to 0.458). Interestingly, our results suggest that Ethiopia’s multidimensional poverty dynamics are more responsive to a reduction in the poverty rates when the MPI increases compared to an increase in the spillover rate above the inferred optimal value. These findings suggest that bottlenecks in spillovers between indicators could substantially affect poverty reduction in Ethiopia.\n",
    "\n",
    "Next, in Figure 3 b, we show the average spillovers (between 2021 and 2030) received by each poverty indicator during the prospective analysis for various values of  r^c as a function of Poverty Centrality. We observe a nearly perfect correlation between these two variables for each choice of  r^c. This indicates that Poverty Centrality effectively reflects the spillovers in PPI simulations. Importantly, it suggests that while Poverty Centrality can identify which indicators are most likely to receive spillovers, the actual volume of spillovers is dependent on the rate   r^c . Thus, Poverty Centrality serves as a relative metric, providing insights into the potential distribution of spillovers among indicators, but the magnitude of these spillovers varies with the spillover rate.\n",
    "\n",
    "Moving on to Figure 3 c, we illustrate the censored headcount ratio of each indicator as a function of the spillover rate. This analysis allows us to see how each poverty indicator’s censored headcount ratio changes as the spillover rate varies. Indicators with the lowest and highest values for the censored headcount ratio show the least susceptibility to changes in the spillover rate, indicating that their the censored headcount ratio remains relatively stable even as  r^c varies. In contrast, indicators with intermediate levels of the censored headcount ratio appear the most susceptible, showing greater fluctuations with changes in  r^c.\n",
    "\n",
    "Figure 3 d provides a more detailed view by plotting the changes in the censored headcount ratio for each indicator as a function of their 2019 levels under different spillover rates. Indeed, two indicators with intermediate levels of CHR in 2019 (Schooling and Drinking Water) display the largest change due to changes in the spillover rate. This result suggests that the responsiveness of indicators with intermediate censored headcount ratio levels to changes in spillover rates might be due to shifts in policy priority driven by spillover effects, at least in Ethiopia. When spillovers are present, resources and attention may be redistributed towards these intermediate indicators, thereby amplifying their susceptibility to changes in the spillover rate. This underscores the importance of considering how policy priorities and resource allocations may shift due to spillover effects. By identifying which indicators are most responsive to these shifts, policymakers can better target their efforts to maximize the impact of resource allocation and intervention strategies, ensuring that critical areas receive the necessary support to reduce poverty effectively.\n",
    "\n",
    "The results are averaged across 100 PPI simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "r_values = np.arange(0, 1.05, 0.05)\n",
    "gof = []\n",
    "# Load and filter data\n",
    "country = 'eth'\n",
    "filtered_df = TT_FINAL[TT_FINAL['country'] == country].copy()\n",
    "\n",
    "# Create initial data frame\n",
    "data = pd.DataFrame()\n",
    "data['seriesCode'] = pd.Series(filtered_df['var_sample'].unique())\n",
    "data['goal'] = 1\n",
    "data['target'] = 1\n",
    "data['type'] = 0\n",
    "data['countryCode'] = country\n",
    "mapping_dict = filtered_df.set_index('var_sample')['full_names'].to_dict()\n",
    "data['seriesName'] = data['seriesCode'].map(mapping_dict)\n",
    "\n",
    "years = pd.to_numeric(filtered_df['year'], errors='coerce')\n",
    "unique_years = np.sort(years.unique())\n",
    "\n",
    "filtered_df_1 = filtered_df[years == unique_years[0]]\n",
    "mapping_dict = filtered_df_1.set_index('var_sample')['A_int'].to_dict()\n",
    "data['1'] = 1 - data['seriesCode'].map(mapping_dict)\n",
    "mapping_dict = filtered_df_1.set_index('var_sample')['centrality_w_norm'].to_dict()\n",
    "data['PC_1'] = data['seriesCode'].map(mapping_dict)\n",
    "\n",
    "filtered_df_2 = filtered_df[years == unique_years[1]]\n",
    "mapping_dict = filtered_df_2.set_index('var_sample')['A_int'].to_dict()\n",
    "data['2'] = 1 - data['seriesCode'].map(mapping_dict)\n",
    "mapping_dict = filtered_df_2.set_index('var_sample')['centrality_w_norm'].to_dict()\n",
    "data['PC_2'] = data['seriesCode'].map(mapping_dict)\n",
    "\n",
    "filtered_df_3 = filtered_df[years == unique_years[2]]\n",
    "mapping_dict = filtered_df_3.set_index('var_sample')['A_int'].to_dict()\n",
    "data['3'] = 1 - data['seriesCode'].map(mapping_dict)\n",
    "mapping_dict = filtered_df_3.set_index('var_sample')['centrality_w_norm'].to_dict()\n",
    "data['PC_3'] = data['seriesCode'].map(mapping_dict)\n",
    "\n",
    "data['instrumental'] = [0, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "mask = np.array(data['instrumental']) == 1\n",
    "\n",
    "data['colors'] = ['#E5243B', '#DDA63A', '#4C9F38', '#C5192D', '#FF3A21', '#26BDE2', '#FCC30B', '#A21942', '#FD6925', '#DD1367']\n",
    "indis_index = dict([(code, i) for i, code in enumerate(data.seriesCode)]) # used to build the network matrix\n",
    "\n",
    "# Load government expenditure data\n",
    "data_exp = pd.read_excel('data/general-government-data.xlsx', sheet_name='cleaned_data')\n",
    "data_wgi = pd.read_excel('data/wgi-data.xlsx', sheet_name='cleaned_data')\n",
    "\n",
    "# Inference parameters\n",
    "qm = float(data_wgi.loc[data_wgi['year'] == 2011, 'qm'].values[0]) # Governance param 1\n",
    "rl = float(data_wgi.loc[data_wgi['year'] == 2011, 'rl'].values[0]) # Governance param 2\n",
    "T = 54\n",
    "\n",
    "\n",
    "\n",
    "years_exp = [column_name for column_name in data_exp.columns if str(column_name).isnumeric()]\n",
    "periods = len(years_exp)\n",
    "t = int(T / periods)\n",
    "div = T / len(years_exp)\n",
    "new_rows = []\n",
    "for index, row in data_exp.iterrows():\n",
    "    new_row = [row.sdg]\n",
    "    for year in years_exp:\n",
    "        new_row += [int(row[year]) / div for i in range(t)]\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "df_exp = pd.DataFrame(new_rows, columns=['sdg'] + list(range(T)))\n",
    "Bs = df_exp.values[:, 1:] # disbursement schedule (assumes that the expenditure programmes are properly sorted)\n",
    "\n",
    "is_instrumental = dict(zip(data.seriesCode, data.instrumental == 1))\n",
    "rel_dict = dict([(code, []) for code in data.seriesCode if is_instrumental[code]])\n",
    "for index, row in data.iterrows():\n",
    "    if row.seriesCode in rel_dict:\n",
    "        rel_dict[row.seriesCode].append(row.type)\n",
    "\n",
    "n_cols = max([len(value) for value in rel_dict.values()])\n",
    "\n",
    "M = [['' for i in range(n_cols + 1)] for code in rel_dict.values()]\n",
    "for i, items in enumerate(rel_dict.items()):\n",
    "    sdg, indis = items\n",
    "    M[i][0] = sdg\n",
    "    for j, indi in enumerate(indis):\n",
    "        M[i][j + 1] = indi\n",
    "\n",
    "df_rel = pd.DataFrame(M, columns=['seriesCode'] + list(range(n_cols)))\n",
    "B_dict = {} # PPI needs the relational table in the form of a Python dictionary\n",
    "for index, row in df_rel.iterrows():\n",
    "    B_dict[indis_index[row.seriesCode]] = [programme for programme in row.values[1:] if str(programme) != 'nan']\n",
    "\n",
    "R = data.instrumental # instrumental indicators\n",
    "\n",
    "# Calibration\n",
    "parallel_processes = 6 # number of cores to use\n",
    "threshold = 0.8 # the quality of the calibration (I choose a medium quality for illustration purposes)\n",
    "low_precision_counts = 100 # number of low-quality iterations to accelerate the calibration\n",
    "\n",
    "# Data for inference\n",
    "I0 = data['1']\n",
    "IF = data['3']\n",
    "years_cols = [col for col in data.columns if col.isnumeric()]\n",
    "successRates = np.sum(data[years_cols].values[:, 1:] > data[years_cols].values[:, :-1], axis=1) / (len(years_cols) - 1)\n",
    "successRates[successRates == 0] = .1\n",
    "successRates[successRates == 1] = .9\n",
    "\n",
    "# Loop over r values and calculate x\n",
    "for r in r_values:\n",
    "    # Update the spillover network A with the current r value\n",
    "    A = PHI_NORMALIZED[f'phi_{country}_{unique_years[0]}'].copy()\n",
    "    A = r * A.T\n",
    "\n",
    "    parameters = ppi.calibrate(I0, IF, successRates, A=A, R=R, qm=qm, rl=rl, Bs=Bs, B_dict=B_dict,\n",
    "                               T=T, threshold=threshold, parallel_processes=parallel_processes, verbose=False,\n",
    "                               low_precision_counts=low_precision_counts)\n",
    "    df_params = pd.DataFrame(parameters[1:], columns=parameters[0])\n",
    "    \n",
    "    error = np.mean(df_params['GoF_alpha'].values.astype(float))\n",
    "    gof.append(error)\n",
    "    \n",
    "    print(f\"r value = {r} and GoF = {error}\")\n",
    "    \n",
    "# Find the r value that corresponds to the maximum GoF\n",
    "max_gof_index = np.argmax(gof)\n",
    "r_opt = r_values[max_gof_index]    \n",
    "    \n",
    "## SIMULATE FIGURE 3 a and b\n",
    "# retrospective\n",
    "A = PHI_NORMALIZED[f'phi_{country}_{unique_years[0]}'].copy()\n",
    "A = r_opt * A.T\n",
    "parameters = ppi.calibrate(I0, IF, successRates, A=A, R=R, qm=qm, rl=rl, Bs=Bs, B_dict=B_dict,\n",
    "                               T=T, threshold=threshold, parallel_processes=parallel_processes, verbose=False,\n",
    "                               low_precision_counts=low_precision_counts)\n",
    "df_params = pd.DataFrame(parameters[1:], columns=parameters[0])\n",
    "\n",
    "# prospective parameters\n",
    "qm = float(data_wgi.loc[data_wgi['year'] == 2019, 'qm'].values[0]) # Governance param 1\n",
    "rl = float(data_wgi.loc[data_wgi['year'] == 2019, 'rl'].values[0]) # Governance param 2\n",
    "sample_size = 100 # number of Monte Carlo simulations\n",
    "Imax = np.ones(len(IF))\n",
    "Imin = np.zeros(len(IF))\n",
    "Tx = 66\n",
    "Bs_x = np.zeros(Tx) + np.mean(Bs)\n",
    "\n",
    "goals = data['goal'].values.astype(float)\n",
    "alphas = df_params.alpha.values.astype(float)\n",
    "alphas_prime = df_params.alpha_prime.values.astype(float)\n",
    "betas = df_params.beta.values.astype(float)\n",
    "\n",
    "CensoredHeadcountRatio = np.full((len(goals), len(r_values)), np.nan)\n",
    "EstimatedSpillovers = np.full((len(goals), len(r_values)), np.nan)\n",
    "EstimatedContributions = np.full((len(goals), len(r_values)), np.nan)\n",
    "EstimatedBenefits = np.full((len(goals), len(r_values)), np.nan)\n",
    "EstimatedAllocations = np.full((len(goals), len(r_values)), np.nan)\n",
    "EstimatedGrowth = np.full((len(goals), len(r_values)), np.nan)\n",
    "\n",
    "for r_index, r in enumerate(r_values):\n",
    "    # Update the spillover network A with the current r value\n",
    "    A = PHI_NORMALIZED[f'phi_{country}_{unique_years[2]}'].copy()\n",
    "    A = r * A.T\n",
    "\n",
    "    outputs = []\n",
    "    for sample in range(sample_size):\n",
    "        output = ppi.run_ppi(IF, alphas, alphas_prime, betas, A=A, R=R, qm=qm, rl=rl,Imax=Imax, Imin=Imin, Bs=Bs_x, B_dict=B_dict, T=Tx, G=goals, seed=sample)\n",
    "        outputs.append(output)\n",
    "\n",
    "    tsI, tsC, tsF, tsP, tsS, tsG = zip(*outputs)\n",
    "    tsI_hat = np.mean(tsI, axis=0)\n",
    "    tsC_hat = np.mean(tsC, axis=0)\n",
    "    tsF_hat = np.mean(tsF, axis=0)\n",
    "    tsP_hat = np.mean(tsP, axis=0)        \n",
    "    tsS_hat = np.mean(tsS, axis=0)\n",
    "    tsG_hat = np.mean(tsG, axis=0) \n",
    "\n",
    "    # Set CensoredHeadcountRatio for each r (column) as the last column of tsI_hat\n",
    "    CensoredHeadcountRatio[:, r_index] = 1-tsI_hat[:, -1]\n",
    "    \n",
    "    # Set the value of EstimatedSpillovers for each r as the sum of each row of tsS_hat\n",
    "    EstimatedSpillovers[:, r_index] = np.mean(tsS_hat, axis=1)\n",
    "    EstimatedContributions[mask, r_index] = np.mean(tsC_hat, axis=1)\n",
    "    EstimatedBenefits[mask, r_index] = np.mean(tsF_hat, axis=1)\n",
    "    EstimatedAllocations[mask, r_index] = np.mean(tsP_hat, axis=1)\n",
    "    EstimatedGrowth[:, r_index] = np.mean(tsG_hat, axis=1)\n",
    "    print(r)\n",
    "    \n",
    "\n",
    "# Calculate the sum of CensoredHeadcountRatio across all goals\n",
    "sum_CensoredHeadcountRatio = np.sum(CensoredHeadcountRatio, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the `1-IF` values and the `CensoredHeadcountRatio` for each indicator\n",
    "one_minus_IF = 1 - IF  # Calculate 1-IF if it's not already provided\n",
    "\n",
    "## FIGURE\n",
    "\n",
    "# Create a 2x2 subplot figure\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=('a', 'b', 'c', 'd'))\n",
    "\n",
    "# Define a list of colors, line styles, and markers\n",
    "colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A', '#19D3F3', '#FF6692', '#B6E880', '#FF97FF', '#FECB52']\n",
    "line_styles = ['solid', 'dash', 'dot', 'dashdot']\n",
    "markers = ['circle', 'diamond', 'triangle-up', 'square']\n",
    "\n",
    "# Add sum of CensoredHeadcountRatio to first subplot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=r_values,\n",
    "    y=sum_CensoredHeadcountRatio,\n",
    "    mode='lines',\n",
    "    name='MPI',\n",
    "    line=dict(color='black', width=9),\n",
    "    showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# Add vertical and horizontal lines to the first subplot\n",
    "fig.add_shape(type=\"line\", x0=r_opt, y0=0, x1=r_opt, y1=np.sum(1-IF), xref='x', yref='y', line=dict(color=\"maroon\", width=3), row=1, col=1)\n",
    "fig.add_shape(type=\"line\", x0=r_values[0], y0=np.sum(1-IF), x1=r_values[-1], y1=np.sum(1-IF), xref='x', yref='y', line=dict(color=\"blue\", width=3, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "# Add each goal's CensoredHeadcountRatio to the second subplot\n",
    "for i in range(len(goals)):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=r_values,\n",
    "        y=CensoredHeadcountRatio[i, :],\n",
    "        mode='lines+markers',\n",
    "        name=data['seriesName'][i],\n",
    "        line=dict(color=colors[i % len(colors)], width=1.5, dash=line_styles[i % len(line_styles)]),\n",
    "        marker=dict(symbol=markers[i % len(markers)], size=6),\n",
    "        showlegend=True\n",
    "    ), row=2, col=1)\n",
    "\n",
    "# Add vertical line to the second subplot\n",
    "fig.add_shape(type=\"line\", x0=r_opt, y0=-0.01, x1=r_opt, y1=np.max(CensoredHeadcountRatio)*1.1, xref='x', yref='y', line=dict(color=\"maroon\", width=3), row=2, col=1)\n",
    "\n",
    "# Sort indicators by 1-IF values\n",
    "sorted_indices = np.argsort(one_minus_IF)\n",
    "sorted_indicators = np.array(data['seriesName'])[sorted_indices]\n",
    "sorted_one_minus_IF = one_minus_IF[sorted_indices]\n",
    "sorted_CensoredHeadcountRatio = CensoredHeadcountRatio[sorted_indices, :]\n",
    "\n",
    "# Bar charts in the fourth subplot\n",
    "r_values_to_plot = [0, 0.3, r_opt, 1]\n",
    "for r_index, r in enumerate(r_values_to_plot):\n",
    "    r_value_index = np.where(np.isclose(r_values, r))[0][0]\n",
    "    y_values = sorted_CensoredHeadcountRatio[:, r_value_index] - sorted_one_minus_IF\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sorted_indicators,\n",
    "        y=y_values,\n",
    "        name=f'r = {r}',\n",
    "        marker=dict(color=colors[r_index % len(colors)]),\n",
    "        showlegend=True\n",
    "    ), row=2, col=2)\n",
    "\n",
    "# Scatter plots in the third subplot\n",
    "for r_index, r in enumerate(r_values_to_plot):\n",
    "    r_value_index = np.where(np.isclose(r_values, r))[0][0]\n",
    "    correlation = np.corrcoef(data['PC_3'], EstimatedSpillovers[:, r_value_index])[0, 1]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data['PC_3'],\n",
    "        y=EstimatedSpillovers[:, r_value_index],\n",
    "        mode='markers',\n",
    "        name=f'r = {r}, Corr = {correlation:.2f}',\n",
    "        marker=dict(size=8, symbol=markers[r_index % len(markers)], line=dict(width=2), color=colors[r_index % len(colors)]),\n",
    "        showlegend=True\n",
    "    ), row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.1, xanchor='center', x=0.5),\n",
    "    margin=dict(l=40, r=40, t=40, b=40)\n",
    ")\n",
    "\n",
    "# Update axes for each subplot\n",
    "axis_params = dict(showline=True, showgrid=True, showticklabels=True, linecolor='black', linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='black'), gridcolor='lightgrey')\n",
    "fig.update_xaxes(**axis_params, title_text='Spillover rate', tickvals=[0, 0.2, 0.4, 0.6, 0.8, 1], row=1, col=1)\n",
    "fig.update_yaxes(**axis_params, title_text='MPI', range=[0.4, np.max(sum_CensoredHeadcountRatio) * 1.1], row=1, col=1)\n",
    "fig.update_xaxes(**axis_params, title_text='Spillover rate', tickvals=[0, 0.2, 0.4, 0.6, 0.8, 1], row=2, col=1)\n",
    "fig.update_yaxes(**axis_params, title_text='CHR', row=2, col=1)\n",
    "fig.update_xaxes(**axis_params, title_text='Indicators', tickvals=list(range(len(sorted_indicators))), row=2, col=2)\n",
    "fig.update_yaxes(**axis_params, title_text='Change in CHR', row=2, col=2)\n",
    "fig.update_xaxes(**axis_params, title_text='Poverty Centrality', row=1, col=2)\n",
    "fig.update_yaxes(**axis_params, title_text='Estimated Spillovers', row=1, col=2)\n",
    "\n",
    "# Export the figure as a high-quality PDF\n",
    "width_cm = 20  # desired width in cm\n",
    "height_cm = 21  # desired height in cm\n",
    "width_px = width_cm * 37.7953\n",
    "height_px = height_cm * 37.7953\n",
    "fig.write_image(\"output/figure3.pdf\", format='pdf', width=width_px, height=height_px, scale=3)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4500b6e",
   "metadata": {},
   "source": [
    "# APPENDIX 3 FIGURE A3.1\n",
    "\n",
    "This figure shows the median poverty centrality by income group. We find that the patterns by income group are similar to the overall patterns discovered in Figure 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MEDIAN POVERTY CENTRALITY BY INCOME GROUP\n",
    "\n",
    "file_path = 'data/OGHIST.xlsx'\n",
    "sheet_name = 'Country Analytical History'\n",
    "\n",
    "df_region = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "df_region['Code'] = df_region['Code'].str.lower()\n",
    "\n",
    "TT_regression_plot = TT_regression.copy()\n",
    "TT_regression_plot['year'] = 2000 + TT_regression_plot['year'].astype(int)\n",
    "TT_regression_plot = TT_regression_plot.merge(df_region[['Code','Year', 'Group']], left_on=['country', 'year'], right_on=['Code','Year'], how='left')\n",
    "\n",
    "# Get unique regions\n",
    "unique_regions = TT_regression_plot['Group'].unique()\n",
    "unique_regions = unique_regions[unique_regions != 'H'] # Drop high income\n",
    "\n",
    "# Mapping unique region codes to their full names\n",
    "region_titles = {\n",
    "    'UM': 'Upper-middle income',\n",
    "    'LM': 'Lower-middle income',\n",
    "    'L': 'Low income'\n",
    "}\n",
    "\n",
    "# Create a subplot with a column for each unique region\n",
    "fig = make_subplots(rows=1, cols=len(unique_regions), shared_yaxes=True, subplot_titles=[region_titles[region] for region in unique_regions])\n",
    "\n",
    "# Loop through each region and create the bar chart\n",
    "for i, region in enumerate(unique_regions):\n",
    "    # Filter the data for the current region\n",
    "    region_data = TT_regression_plot[TT_regression_plot['Group'] == region]\n",
    "    median_indicator = region_data.groupby('full_names')['centrality_w_norm_final'].median().reset_index()\n",
    "    \n",
    "    # Merge with median_indicator_sorted_general to get the correct order\n",
    "    median_indicator_sorted = median_indicator.merge(median_indicator_sorted_general[['full_names']], on='full_names', how='right')\n",
    "    \n",
    "    # Add the bar chart to the subplot\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=median_indicator_sorted['centrality_w_norm_final'],\n",
    "        y=median_indicator_sorted['full_names'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='rgba(31, 119, 180, 0.8)', line=dict(color='DarkSlateGrey', width=1)),\n",
    "        showlegend=False\n",
    "    ), row=1, col=i+1)\n",
    "\n",
    "    # Update x and y axes for the subplot\n",
    "    fig.update_xaxes(title_text='Median Poverty Centrality', showline=True, linewidth=2, linecolor='black',\n",
    "                     titlefont=dict(color='black'), tickfont=dict(color='black'), row=1, col=i+1)\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True,\n",
    "                     titlefont=dict(color='black'), tickfont=dict(color='black'), row=1, col=i+1)\n",
    "\n",
    "# Update layout to set all text to black\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1500,\n",
    "    title_text=\"Median Poverty Centrality by Income Group\",\n",
    "    title_font=dict(color='black'),\n",
    "    font=dict(color='black'),\n",
    "    paper_bgcolor='white',  # Set background color to white\n",
    "    plot_bgcolor='white',  # Set plot area background color to white\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Export the figure as a high-quality PDF\n",
    "pio.write_image(fig, 'output/figureA31.pdf', format='pdf', width=1200, height=500, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75f7cf",
   "metadata": {},
   "source": [
    "# APPENDIX 3 FIGURE A3.2\n",
    "\n",
    "This figure shows the median poverty centrality by region. We find that the patterns by region are different to the overall patterns discovered in Figure 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = 'data/OGHIST.xlsx'\n",
    "sheet_name = 'region'\n",
    "df_region = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "df_region['Code'] = df_region['Code'].str.lower()\n",
    "\n",
    "# Merge with TT_regression to add the Region information\n",
    "TT_regression_plot = TT_regression.copy()\n",
    "TT_regression_plot = TT_regression_plot.merge(df_region[['Code', 'Region']], left_on='country', right_on='Code', how='left')\n",
    "\n",
    "# Get unique regions\n",
    "unique_regions = TT_regression_plot['Region'].unique()\n",
    "\n",
    "# Create a subplot with a column for each unique region\n",
    "fig = make_subplots(rows=1, cols=len(unique_regions), shared_yaxes=True, subplot_titles=unique_regions)\n",
    "\n",
    "# Loop through each region and create the bar chart\n",
    "for i, region in enumerate(unique_regions):\n",
    "    # Filter the data for the current region\n",
    "    region_data = TT_regression_plot[TT_regression_plot['Region'] == region]\n",
    "    median_indicator = region_data.groupby('full_names')['centrality_w_norm_final'].median().reset_index()\n",
    "    \n",
    "    # Merge with median_indicator_sorted_general to get the correct order\n",
    "    median_indicator_sorted = median_indicator.merge(median_indicator_sorted_general[['full_names']], on='full_names', how='right')\n",
    "    \n",
    "    # Add the bar chart to the subplot\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=median_indicator_sorted['centrality_w_norm_final'],\n",
    "        y=median_indicator_sorted['full_names'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='rgba(31, 119, 180, 0.8)', line=dict(color='DarkSlateGrey', width=1)),\n",
    "        showlegend=False\n",
    "    ), row=1, col=i+1)\n",
    "\n",
    "    # Update x and y axes for the subplot\n",
    "    fig.update_xaxes(title_text='Median Poverty Centrality', showline=True, linewidth=2, linecolor='black',\n",
    "                     titlefont=dict(color='black'), tickfont=dict(color='black'), row=1, col=i+1)\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True,\n",
    "                     titlefont=dict(color='black'), tickfont=dict(color='black'), row=1, col=i+1)\n",
    "\n",
    "# Update layout\n",
    "# Update layout to set all text to black\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1500,\n",
    "    title_text=\"Median Poverty Centrality by Region\",\n",
    "    title_font=dict(color='black'),\n",
    "    font=dict(color='black'),\n",
    "    paper_bgcolor='white',  # Set background color to white\n",
    "    plot_bgcolor='white',  # Set plot area background color to white\n",
    ")\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Export the figure as a high-quality PDF\n",
    "pio.write_image(fig, 'output/figureA32.pdf', format='pdf', width=1200, height=500, scale=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fbd99",
   "metadata": {},
   "source": [
    "## APPENDIX 2 FIGURE A2.1.\n",
    "\n",
    "Figure A2.1. Reproduces Figure 2 from the main manuscript, describing the general patterns of the Poverty Space among countries, just instead of using a threshold of being poor in at least one dimension for the definition of multidimensional poverty, we use a threshold of being poor in at least 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d78dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAR DATA: ESTIMATE THE POVERTY SPACE & POVERTY CENTRALITY\n",
    "\n",
    "\n",
    "var_weights = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "variables = ['d_cm_01', 'd_nutr_01', 'd_satt_01', 'd_educ_01', 'd_elct_01',\n",
    "             'd_wtr_01', 'd_sani_01', 'd_hsg_01', 'd_ckfl_01', 'd_asst_01']\n",
    "dimensions = [1, 1, 2, 2, 3, 3, 3, 3, 3, 3]\n",
    "full_variable_names = ['Child mortality', 'Nutrition', 'School attendance', 'Schooling',\n",
    "                       'Electricity', 'Drinking water', 'Sanitation', 'Housing',\n",
    "                       'Cooking fuel', 'Assets']\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "# In Python, using glob, you can search for all CSV files in the current working directory\n",
    "\n",
    "search_pattern = os.path.join(os.getcwd(), 'data', '*.csv')\n",
    "csv_files = glob.glob(search_pattern)\n",
    "\n",
    "PHI_NORMALIZED = {}\n",
    "list_names = {}\n",
    "list_short_names = {}\n",
    "\n",
    "Correlation_matrix = np.nan * np.zeros((len(csv_files), len(variables)))\n",
    "\n",
    "                                       \n",
    "columns = ['country', 'year', 'MPI', 'var_sample', 'A_int', 'A_avg', 'A_avg_raw', 'A_int_rank', 'centrality_w_norm', 'centrality_w_norm_rank', 'full_names']\n",
    "TT_FINAL = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in range(len(csv_files)):\n",
    "\n",
    "    # Get the file name from the path\n",
    "    filename = os.path.basename(csv_files[i])\n",
    "\n",
    "    # Extract country code and year from the filename\n",
    "    ctry = filename[:3]\n",
    "    yyr = filename[-6:-4]  # Adjust according to your file naming convention if needed\n",
    "\n",
    "    # Read the data\n",
    "    TT = pd.read_csv(csv_files[i])\n",
    "\n",
    "    # Create the deprivation matrix\n",
    "    XX = pd.DataFrame()\n",
    "    jj = []\n",
    "\n",
    "    for j, var in enumerate(variables):\n",
    "        if var in TT.columns:\n",
    "            X = TT[var]\n",
    "            if X.isna().mean() < 1:  # Check if the column is not entirely NaN\n",
    "                XX = pd.concat([XX, X], axis=1)\n",
    "            else:\n",
    "                jj.append(j)  # Append j+1 to maintain MATLAB's 1-based indexing\n",
    "\n",
    "    # Assuming previous steps have been correctly executed\n",
    "\n",
    "    # Convert XX back to DataFrame if not already, to simplify handling NaN values\n",
    "    # This step is precautionary; XX should already be a DataFrame after previous operations\n",
    "    XX = pd.DataFrame(XX)\n",
    "\n",
    "    # Filter out rows with any NaN values across all columns in XX\n",
    "    # Also filter corresponding rows in the weights array\n",
    "    valid_rows = ~XX.isnull().any(axis=1)\n",
    "    XX_filtered = XX[valid_rows]\n",
    "\n",
    "    weights = TT['weight'].values  # Extracting weight values from the DataFrame\n",
    "    weights_sample = np.array(var_weights)  # Ensure var_weights is a numpy array for easy manipulation\n",
    "\n",
    "    # Adjust weights_sample based on jj, ensuring jj is zero-based\n",
    "    weights_sample = np.delete(weights_sample, jj)\n",
    "\n",
    "    weights_filtered = weights[valid_rows]\n",
    "\n",
    "    var_sample = [var for i, var in enumerate(variables) if i not in jj]\n",
    "    full_names = [name for i, name in enumerate(full_variable_names) if i not in jj]\n",
    "\n",
    "\n",
    "    \n",
    "    # Normalize the sample weights\n",
    "    weights_sample_normalized = weights_sample / sum(weights_sample)\n",
    "\n",
    "    # Calculate the weighted deprivation matrix G\n",
    "    # Note: Element-wise multiplication of weights_sample_normalized with each column in XX_filtered\n",
    "    G = XX_filtered.multiply(weights_sample_normalized, axis='columns')\n",
    "\n",
    "    # Calculate Z based on the threshold condition\n",
    "    Z = np.where(G.sum(axis=1) > threshold, 1, 0) if threshold == 0 else np.where(G.sum(axis=1) >= threshold, 1, 0)\n",
    "\n",
    "    # Calculate the final index H\n",
    "    H = np.sum(weights_filtered * Z) / np.sum(weights_filtered)    \n",
    "\n",
    "\n",
    "    # Ensure G is a numpy array for these operations, if not already\n",
    "    G = G.to_numpy() if hasattr(G, 'to_numpy') else G\n",
    "\n",
    "    # Recreate G_k from G\n",
    "    G_k = np.copy(G)\n",
    "\n",
    "    # Initialize xx with zeros with the correct dimensions\n",
    "    # Note: Adjusting approach to ensure dimensions match expectations for numpy operations\n",
    "    xx = np.zeros((np.sum(1 - Z), G_k.shape[1]))\n",
    "\n",
    "    # Update G_k directly with numpy where applicable\n",
    "    # We need to ensure we're updating rows where Z is 0 with xx, but this approach requires boolean indexing or equivalent in numpy\n",
    "    not_Z_rows = np.where(Z == 0)[0]  # Get indices of rows where Z is 0\n",
    "\n",
    "    # Since direct assignment as attempted can mismatch dimensions, we ensure alignment before attempting to replace\n",
    "    # However, given xx aims to replace all not_Z_rows with zeros, we can simplify by directly setting zeros without using xx\n",
    "    G_k[not_Z_rows, :] = 0  # Directly set rows where Z is 0 to zeros, aligning with intention\n",
    "\n",
    "    # Proceed with adjusted calculations\n",
    "    # Note: The calculations for W, A, A_est, and MPI should then ensure they are using numpy operations as well\n",
    "\n",
    "    W_filtered = weights_filtered.reshape(-1, 1)  # Ensure weights_filtered is column vector for matrix operations\n",
    "    A = (W_filtered * G_k) / np.sum(weights_filtered[Z == 1])\n",
    "\n",
    "    A_est = np.sum(A, axis=0)\n",
    "\n",
    "    MPI = np.sum(A_est * H)\n",
    "\n",
    "    A_est *= H  # Update A_est by multiplying it with H\n",
    "\n",
    "    # Create weighted networks\n",
    "    Z_indices = np.where(Z)[0]  # Find indices where Z is True\n",
    "    G_z = XX_filtered.iloc[Z_indices, :].to_numpy()  # Filter G_z based on Z\n",
    "\n",
    "    # Ensure 'weights' and 'Z' are numpy arrays for the operations\n",
    "    weights_final = np.array(weights_filtered)\n",
    "    Z = np.array(Z, dtype=bool)\n",
    "    A = np.array(A)  # Assuming 'A' is already correctly shaped and matches 'Z'\n",
    "\n",
    "    # Select rows from 'A' where 'Z' is True\n",
    "    A_filtered = A[Z, :]\n",
    "\n",
    "    # Calculate the dot product of 'weights' transposed and 'Z' (which should be a scalar if both are vectors)\n",
    "    weights_Z_product = np.dot(weights_final.T, Z)\n",
    "\n",
    "    # Scale 'A_filtered' by 'weights_Z_product'\n",
    "    M_w = weights_Z_product * A_filtered ## NOT CALCULATED CORRECTLY!!!!\n",
    "\n",
    "    # Remove columns where the sum across rows in G_z is 0\n",
    "    xx = np.sum(G_z > 0, axis=0)\n",
    "    jj = np.where(xx == 0)[0]\n",
    "\n",
    "    # Update var_sample and full_names by removing indices jj\n",
    "    var_sample = np.delete(np.array(var_sample), jj).tolist()\n",
    "    full_names = np.delete(np.array(full_names), jj).tolist()\n",
    "\n",
    "    # Update G_z, M_w, and M_u by removing columns jj\n",
    "    G_z = np.delete(G_z, jj, axis=1)\n",
    "    M_w = np.delete(M_w, jj, axis=1)\n",
    "\n",
    "    # Create similarity networks\n",
    "    PHI_w_int = np.dot(M_w.T, G_z)  # Calculate initial similarity matrix\n",
    "    PHI_w_norm = np.full(PHI_w_int.shape, np.nan)  # Initialize PHI_w_norm with NaN\n",
    "\n",
    "    # Normalize PHI_w_int to create PHI_w_norm\n",
    "    for k in range(PHI_w_int.shape[0]):\n",
    "        for j in range(PHI_w_int.shape[1]):\n",
    "            PHI_w_norm[k, j] = PHI_w_int[k, j] / PHI_w_int[j, j]\n",
    "\n",
    "    # Adjust PHI_w_norm by subtracting the identity matrix scaled by PHI_w_norm\n",
    "    PHI_w_norm = PHI_w_norm - np.eye(PHI_w_norm.shape[0]) * PHI_w_norm\n",
    "\n",
    "    D, V = np.linalg.eig(PHI_w_norm)\n",
    "\n",
    "    # Find the index of the eigenvalue with the maximum magnitude\n",
    "    z = np.argmax(D)\n",
    "\n",
    "    # Extract the corresponding eigenvector and compute the centrality measure\n",
    "    centrality_w_norm = V[:, z]\n",
    "\n",
    "    # Normalize the centrality measure so that its sum equals 1\n",
    "    centrality_w_norm /= np.sum(centrality_w_norm)\n",
    "    \n",
    "    for j in range(len(variables)):\n",
    "        \n",
    "        try:\n",
    "            index = var_sample.index(variables[j])\n",
    "        except ValueError:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        Phi_reduced = np.delete(PHI_w_norm, index, axis=0).copy()  # Remove row\n",
    "        Phi_reduced = np.delete(Phi_reduced, index, axis=1)  # Remove column\n",
    "        centrality_reduced = np.delete(centrality_w_norm, index).copy()\n",
    "        \n",
    "        D, V = np.linalg.eig(Phi_reduced)\n",
    "\n",
    "        # Find the index of the eigenvalue with the maximum magnitude\n",
    "        z = np.argmax(D)\n",
    "\n",
    "        # Extract the corresponding eigenvector and compute the centrality measure\n",
    "        centrality_w_norm_reduced = V[:, z]\n",
    "\n",
    "        # Normalize the centrality measure so that its sum equals 1\n",
    "        centrality_w_norm_reduced /= np.sum(centrality_w_norm_reduced)\n",
    "        centrality_reduced /= np.sum(centrality_reduced)\n",
    "        \n",
    "        \n",
    "        # Calculate the correlation and assign it to the Correlation_matrix\n",
    "        corr_matrix = np.corrcoef(centrality_w_norm_reduced, centrality_reduced)\n",
    "        # corr_matrix[0, 1] or corr_matrix[1, 0] contains the correlation coefficient\n",
    "        Correlation_matrix[i, j] = corr_matrix[0, 1]\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    # Assuming ctry, yyr, MPI, var_sample, centrality_w_norm, A_est, PHI_w_norm, and full_names are defined\n",
    "\n",
    "    num_vars = len(var_sample)\n",
    "\n",
    "    # 1. Replicating values\n",
    "    country = np.repeat(ctry, num_vars)\n",
    "    year = np.repeat(yyr, num_vars)\n",
    "    MPI_repeated = np.repeat(MPI, num_vars)\n",
    "\n",
    "    # 2. Sorting and ranking centrality_w_norm\n",
    "    centrality_w_norm_rank = rankdata(-centrality_w_norm, method='ordinal')\n",
    "\n",
    "\n",
    "    # Sorting A_int and ranking\n",
    "    A_int = A_est.T\n",
    "    A_int = np.delete(A_int, jj)  # Remove indices as specified by jj\n",
    "    A_int_rank = rankdata(-A_int, method='ordinal')\n",
    "\n",
    "\n",
    "    # 3. Calculating Averages\n",
    "    A_avg = PHI_w_norm @ A_int\n",
    "    PHI_ones = np.ones_like(PHI_w_norm)\n",
    "    PHI_ones -= np.eye(len(A_avg))\n",
    "    A_avg_raw = PHI_ones @ A_int\n",
    "\n",
    "    # 4. Creating the results table\n",
    "    # Note: var_sample, centrality_w_norm, and full_names should be lists or arrays of matching length\n",
    "    TT_results = pd.DataFrame({\n",
    "        'country': country,\n",
    "        'year': year,\n",
    "        'MPI': MPI_repeated,\n",
    "        'var_sample': var_sample,\n",
    "        'A_int': A_int,\n",
    "        'A_avg': A_avg,\n",
    "        'A_avg_raw': A_avg_raw,\n",
    "        'A_int_rank': A_int_rank,  # Adjust for zero-based indexing in Python\n",
    "        'centrality_w_norm': centrality_w_norm,\n",
    "        'centrality_w_norm_rank': centrality_w_norm_rank,\n",
    "        'full_names': full_names\n",
    "    })\n",
    "\n",
    "    # Dynamically setting values in dictionaries using the current value of i\n",
    "    PHI_NORMALIZED[f'phi_{ctry}_{yyr}'] = PHI_w_norm\n",
    "    list_names[f'variable_names_{ctry}_{yyr}'] = full_names\n",
    "    list_short_names[f'variable_names_{ctry}_{yyr}'] = var_sample\n",
    "\n",
    "\n",
    "    TT_FINAL = pd.concat([TT_FINAL, TT_results], ignore_index=True)\n",
    "    \n",
    "print('FINISHED')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## GENERATE DATA FOR REGRESSION ANALYSIS - TABLE 1\n",
    "\n",
    "unique_pairs = TT_FINAL[['country', 'year']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Count occurrences of each unique entry in 'country'\n",
    "count_occurrences = TT_FINAL['country'].value_counts()\n",
    "\n",
    "# Initialize an empty DataFrame for regression data\n",
    "TT_regression = pd.DataFrame()\n",
    "\n",
    "# Iterate over each unique country with more than 10 occurrences\n",
    "for country in count_occurrences[count_occurrences > 10].index:\n",
    "    \n",
    "    # Filter TT_FINAL for the current country\n",
    "    TT_int = TT_FINAL[TT_FINAL['country'] == country].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Convert 'year' to numeric and get unique years\n",
    "    years = pd.to_numeric(TT_int['year'], errors='coerce')\n",
    "    unique_years = years.unique()\n",
    "    \n",
    "    # Find the minimum and maximum years\n",
    "    min_year = TT_int['year'].min()\n",
    "    max_year = TT_int['year'].max()\n",
    "    \n",
    "    # Find all indices where 'year' is equal to the min_year and max_year\n",
    "    z_initial = TT_int[TT_int['year'] == min_year].index\n",
    "    z_final = TT_int[TT_int['year'] == max_year].index\n",
    "    \n",
    "    \n",
    "    names_initial = TT_int.loc[z_initial, 'var_sample']\n",
    "    names_final = TT_int.loc[z_final, 'var_sample']\n",
    "\n",
    "    # Finding the intersection of 'var_sample' names between initial and final\n",
    "    vars = set(names_initial) & set(names_final)\n",
    "\n",
    "    # Define empty lists to store the indices\n",
    "    zz_initial = []\n",
    "    zz_final = []\n",
    "\n",
    "    # Iterate over each variable name in 'vars' to find its occurrences within the initial and final years\n",
    "    for var in vars:\n",
    "        # Filter TT_int for rows where 'var_sample' matches the current variable name\n",
    "        matching_rows = TT_int[TT_int['var_sample'] == var].index\n",
    "\n",
    "        # Intersect matching_rows with z_initial and z_final using pandas Index intersection\n",
    "        intersection_initial = matching_rows.intersection(z_initial)\n",
    "        intersection_final = matching_rows.intersection(z_final)\n",
    "\n",
    "        # Store the first index of the intersection if it exists; otherwise, store NaN\n",
    "        zz_initial.append(intersection_initial[0] if not intersection_initial.empty else np.nan)\n",
    "        zz_final.append(intersection_final[0] if not intersection_final.empty else np.nan)\n",
    "\n",
    "    # Convert lists to pandas Index for further consistency in indexing if needed\n",
    "    zz_initial = pd.Index(zz_initial).dropna().astype(int)\n",
    "    zz_final = pd.Index(zz_final).dropna().astype(int)\n",
    "            \n",
    "    # Extract the initial country-year pair\n",
    "    initial_pair = TT_int.iloc[z_initial][['country', 'year']]\n",
    "\n",
    "    # Assuming 'unique_pairs' is a DataFrame with a reset index\n",
    "    # Find the row in 'unique_pairs' that matches 'initial_pair'\n",
    "    phi_loc = unique_pairs.reset_index().merge(initial_pair, on=['country', 'year'], how='inner')['index'].values[0]\n",
    "\n",
    "    # Using the found index (phi_loc) to access the relevant data\n",
    "    PHI_initial_key = f'phi_{country}_{min_year}'\n",
    "    short_var_name_key = f'variable_names_{country}_{min_year}'\n",
    "\n",
    "    PHI_initial = PHI_NORMALIZED.get(PHI_initial_key, None)\n",
    "    short_var_names = list_short_names.get(short_var_name_key, [])\n",
    "\n",
    "    # Filter PHI_initial based on the intersection of vars and short_var_names\n",
    "    if PHI_initial is not None:\n",
    "        # Find indices of vars in short_var_names\n",
    "        indices = [short_var_names.index(var) for var in vars if var in short_var_names]\n",
    "\n",
    "        # Filter PHI_initial using numpy's advanced indexing\n",
    "        PHI_initial_filtered = PHI_initial[np.ix_(indices, indices)]\n",
    "        \n",
    "        \n",
    "    # Calculate changes for each metric between initial and final observations\n",
    "    change_MPI = TT_int.loc[zz_final, 'MPI'].values - TT_int.loc[zz_initial, 'MPI'].values\n",
    "    change_A = TT_int.loc[zz_final, 'A_int'].values - TT_int.loc[zz_initial, 'A_int'].values\n",
    "    centrality_w_norm_final = TT_int.loc[zz_final, 'centrality_w_norm'].values\n",
    "    # Assuming change_A is a single value or a numpy array that matches the dimensions for multiplication with PHI_initial\n",
    "    # If PHI_initial is a DataFrame, ensure it's converted to a numpy array for this operation, if not already compatible\n",
    "    \n",
    "    # Calculate the number of years between the initial and final observations\n",
    "    # Assuming unique_yyrs is a numpy array or a pandas Series of unique years present in TT_int\n",
    "    change_years = max(unique_years) - min(unique_years)\n",
    "\n",
    "    # Normalize the changes by the number of years\n",
    "    # These operations assume change_years is a scalar. If change_years is an array, adjustments might be needed\n",
    "    change_A /= change_years\n",
    "    \n",
    "    \n",
    "    # Prepare the DataFrame for regression analysis by selecting the initial observation\n",
    "    TT_int_for_regression = TT_int.loc[zz_initial, :].copy()\n",
    "\n",
    "    # Add calculated changes to the DataFrame\n",
    "    TT_int_for_regression['change_MPI'] = change_MPI\n",
    "    TT_int_for_regression['change_A'] = change_A\n",
    "    TT_int_for_regression['change_years'] = change_years\n",
    "    TT_int_for_regression['centrality_w_norm_final'] = centrality_w_norm_final\n",
    "\n",
    "    # Append the prepared DataFrame to the TT_regression DataFrame\n",
    "    TT_regression = pd.concat([TT_regression, TT_int_for_regression], ignore_index=True)\n",
    "    \n",
    "\n",
    "## ADD INSTRUMENTAL\n",
    "\n",
    "\n",
    "unique_regression_pairs = TT_regression[['country', 'year']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Initialize a matrix to hold the instrumental correlations\n",
    "CORRELATION_INSTRUMENTAL = np.nan * np.zeros((len(unique_regression_pairs), len(unique_regression_pairs)))\n",
    "\n",
    "for i in range(len(unique_regression_pairs)):\n",
    "    for j in range(len(unique_regression_pairs)):\n",
    "        # Dynamically construct keys based on country and year\n",
    "        key_i = f\"phi_{unique_regression_pairs.iloc[i]['country']}_{unique_regression_pairs.iloc[i]['year']}\"\n",
    "        key_j = f\"phi_{unique_regression_pairs.iloc[j]['country']}_{unique_regression_pairs.iloc[j]['year']}\"\n",
    "        names_key_i = f\"variable_names_{unique_regression_pairs.iloc[i]['country']}_{unique_regression_pairs.iloc[i]['year']}\"\n",
    "        names_key_j = f\"variable_names_{unique_regression_pairs.iloc[j]['country']}_{unique_regression_pairs.iloc[j]['year']}\"\n",
    "\n",
    "        \n",
    "        PHI_i = PHI_NORMALIZED[key_i]\n",
    "        names_i = list_short_names[names_key_i]\n",
    "\n",
    "        PHI_j = PHI_NORMALIZED[key_j]\n",
    "        names_j = list_short_names[names_key_j]\n",
    "\n",
    "        # Find common variables\n",
    "        vars = list(set(names_i) & set(names_j))\n",
    "        \n",
    "        # Indices of common variables in PHI matrices\n",
    "        zz_1 = [names_i.index(var) for var in vars]\n",
    "        zz_2 = [names_j.index(var) for var in vars]\n",
    "        \n",
    "        # Selecting the relevant rows and columns from PHI matrices based on common variables\n",
    "        PHI_i_reduced = PHI_i[np.ix_(zz_1, zz_1)]\n",
    "        PHI_j_reduced = PHI_j[np.ix_(zz_2, zz_2)]\n",
    "        \n",
    "        # Calculate correlation between flattened matrices\n",
    "        corr_value = np.corrcoef(PHI_i_reduced.flatten(), PHI_j_reduced.flatten())[0, 1]\n",
    "        CORRELATION_INSTRUMENTAL[i, j] = corr_value\n",
    "\n",
    "  #  print(f\"Processed pair {i+1} of {len(unique_regression_pairs)}\")\n",
    "        \n",
    "# Adjust the correlation matrix\n",
    "CORRELATION_INSTRUMENTAL -= np.eye(len(unique_regression_pairs))\n",
    "\n",
    "# Find max correlation indices\n",
    "max_indices = np.argmax(CORRELATION_INSTRUMENTAL, axis=1)\n",
    "\n",
    "# Initialize centrality_w_norm_inst as NaNs\n",
    "centrality_w_norm_inst = np.nan * np.ones(len(TT_regression['centrality_w_norm']))\n",
    "\n",
    "for i in range(len(unique_regression_pairs)):\n",
    "    country_i, year_i = unique_regression_pairs.iloc[i]['country'], unique_regression_pairs.iloc[i]['year']\n",
    "    country_j, year_j = unique_regression_pairs.iloc[max_indices[i]]['country'], unique_regression_pairs.iloc[max_indices[i]]['year']\n",
    "\n",
    "    # Filter TT_regression_2 for the current and max correlated pairs\n",
    "    pair_i = TT_regression[(TT_regression['country'] == country_i) & (TT_regression['year'] == year_i)]\n",
    "    pair_j = TT_regression[(TT_regression['country'] == country_j) & (TT_regression['year'] == year_j)]\n",
    "\n",
    "    # Common variables\n",
    "    vars = list(set(pair_i['var_sample']) & set(pair_j['var_sample']))\n",
    "\n",
    "    # Find indices in TT_regression_2 for vars in both pairs\n",
    "    for var in vars:\n",
    "        index_i = pair_i[pair_i['var_sample'] == var].index\n",
    "        index_j = pair_j[pair_j['var_sample'] == var].index\n",
    "\n",
    "        # Update centrality_w_norm_inst based on common variables\n",
    "        centrality_w_norm_inst[index_i] = pair_j.loc[index_j, 'centrality_w_norm'].values\n",
    "    \n",
    "TT_regression['centrality_w_norm_inst'] = centrality_w_norm_inst\n",
    "\n",
    "## GENERATE DATA FOR 3 PERIOD REGRESSION\n",
    "\n",
    "# Get unique pairs from the first two columns\n",
    "unique_pairs = TT_FINAL.iloc[:, 0:2].drop_duplicates()\n",
    "\n",
    "# Convert country column to string type, find unique entries and their counts\n",
    "unique_entries, entry_counts = np.unique(TT_FINAL['country'].astype(str), return_counts=True)\n",
    "count_occurrences = np.bincount(entry_counts)\n",
    "\n",
    "TT_regression_3_period = pd.DataFrame()\n",
    "for i, unique_entry in enumerate(unique_entries):\n",
    "    if (unique_pairs['country'] == unique_entry).sum() == 3:\n",
    "        indices = TT_FINAL[TT_FINAL['country'] == unique_entry].index\n",
    "        TT_int = TT_FINAL.loc[indices]\n",
    "\n",
    "        # Convert 'year' to numeric and get unique years\n",
    "        years = pd.to_numeric(TT_int['year'], errors='coerce')\n",
    "        unique_years = np.sort(years.unique())\n",
    "    \n",
    "\n",
    "        for KK in range(len(unique_years) - 1):\n",
    "            indices_initial = TT_int[years == unique_years[KK]].index\n",
    "            indices_final = TT_int[years == unique_years[KK + 1]].index\n",
    "\n",
    "            names_initial = TT_int.loc[indices_initial, 'var_sample']\n",
    "            names_final = TT_int.loc[indices_final, 'var_sample']\n",
    "\n",
    "            vars = np.intersect1d(names_initial, names_final)\n",
    "            zz_initial = np.full(vars.shape, np.nan)\n",
    "            zz_final = np.full(vars.shape, np.nan)\n",
    "\n",
    "            for j, var in enumerate(vars):\n",
    "                z = TT_int[TT_int['var_sample'] == var].index\n",
    "                zz_initial[j] = np.intersect1d(indices_initial, z)\n",
    "                zz_final[j] = np.intersect1d(indices_final, z)\n",
    "\n",
    "            initial_pair = TT_int.loc[indices_initial, :].iloc[:, 0:2].drop_duplicates()\n",
    "            \n",
    "             # Using the found index (phi_loc) to access the relevant data\n",
    "            PHI_initial_key = f'phi_{country}_{unique_years[KK]}'\n",
    "            short_var_name_key = f'variable_names_{country}_{unique_years[KK]}'\n",
    "\n",
    "            PHI_initial = PHI_NORMALIZED.get(PHI_initial_key, None)\n",
    "            short_var_names = list_short_names.get(short_var_name_key, [])\n",
    "\n",
    "            # Filter PHI_initial based on the intersection of vars and short_var_names\n",
    "            if PHI_initial is not None:\n",
    "                # Find indices of vars in short_var_names\n",
    "                indices = [short_var_names.index(var) for var in vars if var in short_var_names]\n",
    "\n",
    "                # Filter PHI_initial using numpy's advanced indexing\n",
    "                PHI_initial_filtered = PHI_initial[np.ix_(indices, indices)]\n",
    "                \n",
    "                    # Calculate changes for each metric between initial and final observations\n",
    "            change_MPI = TT_int.loc[zz_final, 'MPI'].values - TT_int.loc[zz_initial, 'MPI'].values\n",
    "            change_A = TT_int.loc[zz_final, 'A_int'].values - TT_int.loc[zz_initial, 'A_int'].values\n",
    "\n",
    "\n",
    "            # Calculate the number of years between the initial and final observations\n",
    "            # Assuming unique_yyrs is a numpy array or a pandas Series of unique years present in TT_int\n",
    "            change_years = unique_years[KK+1] - unique_years[KK]\n",
    "            \n",
    "\n",
    "            # Normalize the changes by the number of years\n",
    "            # These operations assume change_years is a scalar. If change_years is an array, adjustments might be needed\n",
    "            change_A /= change_years\n",
    "\n",
    "\n",
    "            # Prepare the DataFrame for regression analysis by selecting the initial observation\n",
    "            TT_int_for_regression = TT_int.loc[zz_initial, :].copy()\n",
    "\n",
    "            # Add calculated changes to the DataFrame\n",
    "            TT_int_for_regression['change_MPI'] = change_MPI\n",
    "            TT_int_for_regression['change_A'] = change_A\n",
    "            TT_int_for_regression['change_years'] = change_years\n",
    "            TT_int_for_regression['period'] = KK            \n",
    "\n",
    "            # Append the prepared DataFrame to the TT_regression DataFrame\n",
    "            TT_regression_3_period = pd.concat([TT_regression_3_period, TT_int_for_regression], ignore_index=True)\n",
    "            \n",
    "            \n",
    "print('FINISHED')            \n",
    "\n",
    "# Figure 2A\n",
    "correlation_coef = np.corrcoef(TT_FINAL['centrality_w_norm'], TT_FINAL['A_int'])[0, 1]\n",
    "slope, intercept, r_value, p_value, std_err = linregress(TT_FINAL['centrality_w_norm'], TT_FINAL['A_int'])\n",
    "regression_line = slope * TT_FINAL['centrality_w_norm'] + intercept\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3, subplot_titles=('a', 'b', 'c', 'd'), horizontal_spacing=0.08, vertical_spacing=0.15,\n",
    "                    specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "                           [{\"colspan\": 3}, None, None]])\n",
    "\n",
    "# Scatter plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=TT_FINAL['centrality_w_norm'],\n",
    "    y=TT_FINAL['A_int'],\n",
    "    mode='markers',\n",
    "    marker=dict(color='rgba(31, 119, 180, 0.8)', size=8, line=dict(color='DarkSlateGrey', width=1)),\n",
    "    name='Data Points'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Regression line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=TT_FINAL['centrality_w_norm'],\n",
    "    y=regression_line,\n",
    "    mode='lines',\n",
    "    line=dict(color='rgba(255, 0, 0, 0.8)', width=2, dash='dash'),\n",
    "    name='Regression Line'\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text='Poverty Centrality', showline=True, linewidth=2, linecolor='black', row=1, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(title_text='Censored Headcount Ratio', showline=True, linewidth=2, linecolor='black', row=1, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.01, y=0.98, xref='paper', yref='paper',\n",
    "    text=f'Correlation Coefficient: {correlation_coef:.2f}',\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color='black'),\n",
    "    bgcolor='white',\n",
    "    opacity=1\n",
    ")\n",
    "\n",
    "# Figure 2B\n",
    "median_indicator = TT_regression.groupby('full_names')['centrality_w_norm_final'].median().reset_index()\n",
    "median_indicator_sorted_general = median_indicator.sort_values(by='centrality_w_norm_final')\n",
    "\n",
    "# Horizontal bar plot\n",
    "fig.add_trace(go.Bar(\n",
    "    x=median_indicator_sorted_general['centrality_w_norm_final'],\n",
    "    y=median_indicator_sorted_general['full_names'],\n",
    "    orientation='h',\n",
    "    marker=dict(color='rgba(31, 119, 180, 0.8)', line=dict(color='DarkSlateGrey', width=1))\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Median Poverty Centrality', showline=True, linewidth=2, linecolor='black', row=1, col=2,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, row=1, col=2,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "# Figure 2C\n",
    "sorted_full_variable_names = median_indicator_sorted_general['full_names'].tolist()\n",
    "sorted_indices = [full_variable_names.index(name) for name in sorted_full_variable_names]\n",
    "\n",
    "for i in range(len(sorted_indices)):\n",
    "    column_data = Correlation_matrix[:, sorted_indices[i]]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=column_data[~np.isnan(column_data)],\n",
    "        name=sorted_full_variable_names[i],\n",
    "        orientation='h',\n",
    "        boxmean='sd',\n",
    "        marker_color='rgba(31, 119, 180, 0.8)'\n",
    "    ), row=1, col=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=1000,\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Correlation Coefficient', showline=True, linewidth=2, linecolor='black', row=1, col=3,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(showticklabels=False, row=1, col=3)  # Hide y-axis labels for 2C\n",
    "\n",
    "# Align y-axis labels for 2B and 2C\n",
    "fig.update_yaxes(tickmode='array', tickvals=list(range(len(sorted_full_variable_names))), ticktext=sorted_full_variable_names, row=1, col=2,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "# Figure 2D\n",
    "unique_countries = TT_regression['country'].unique()\n",
    "\n",
    "# Initialize a list to store correlation values for each country\n",
    "corr_indicators = []\n",
    "\n",
    "# Calculate the correlation for 'centrality_w_norm' and 'centrality_w_norm_final' for each country\n",
    "for country in unique_countries:\n",
    "    # Filter the DataFrame for the current country\n",
    "    filtered_df = TT_regression[TT_regression['country'] == country]\n",
    "    \n",
    "    # Calculate correlation and append to list\n",
    "    correlation = filtered_df['centrality_w_norm'].corr(filtered_df['centrality_w_norm_final'])\n",
    "    corr_indicators.append(correlation)\n",
    "\n",
    "# Convert the list of correlations to a NumPy array for sorting\n",
    "corr_indicators = np.array(corr_indicators)\n",
    "\n",
    "# Sort the correlations and get the sorted indices\n",
    "sorted_indices = np.argsort(corr_indicators)\n",
    "\n",
    "# Vertical bar plot for correlations\n",
    "fig.add_trace(go.Bar(\n",
    "    x=np.array(unique_countries)[sorted_indices],\n",
    "    y=corr_indicators[sorted_indices],\n",
    "    marker=dict(color='rgba(31, 119, 180, 0.8)', line=dict(color='DarkSlateGrey', width=1))\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text='Country', showline=True, linewidth=2, linecolor='black', row=2, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "fig.update_yaxes(title_text='Poverty centrality correlation in initial and final years of survey', showline=True, linewidth=2, linecolor='black', row=2, col=1,\n",
    "                 titlefont=dict(color='black'), tickfont=dict(color='black'))\n",
    "\n",
    "fig.update_layout(height=800)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Export the figure as a high-quality PDF\n",
    "pio.write_image(fig, 'output/figureA21.pdf', format='pdf', width=1200, height=800, scale=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
